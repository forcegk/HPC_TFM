{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neccesary packages (install them if not installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import os\n",
    "import sys\n",
    "import struct\n",
    "from datetime import date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load('german_credit_numeric', split='train',as_supervised=True)\n",
    "\n",
    "tamano_lote = 1000\n",
    "\n",
    "elems = ds.batch(tamano_lote)\n",
    "lote_entrenamiento = None\n",
    "for elem in elems:\n",
    "    lote_entrenamiento = elem\n",
    "    break\n",
    "\n",
    "# We can also print it, in case we need some information and/or context about it\n",
    "# print(lote_entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set loss function, optimizer and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_perdida = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizador = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "metrica = tf.keras.metrics.Precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasonably sized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamano_entrada = 24\n",
    "h0_size = 5\n",
    "h1_size = 3\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential(name= \"MySampleModel\")\n",
    "model.add( tf.keras.layers.InputLayer((tamano_entrada,)))\n",
    "# Here we add N layers to the model.\n",
    "model.add(tf.keras.layers.Dense(units = h0_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = h1_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = 1, activation=\"sigmoid\"))\n",
    "\n",
    "# Build (could also show and plot) model\n",
    "model.build()\n",
    "# print(model.summary())\n",
    "# tf.keras.utils.plot_model(model, 'MySampleModel.png')\n",
    "\n",
    "# And compile the model\n",
    "model.compile(loss=fn_perdida, optimizer=optimizador, metrics=metrica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overdimensioned model\n",
    "This model is just for trying to stress test the already very optimized BLAS routines. It does not produce any meaningful output, as the easiest way of making the network work is just >0.5 every single input, but it gets the job done in terms of codegen.\n",
    "\n",
    "This should be also quite useful on showing the benefits of pruning on large networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamano_entrada = 24\n",
    "h0_size = 500\n",
    "h1_size = 800\n",
    "h2_size = 1000\n",
    "h3_size = 1200\n",
    "h4_size = 600\n",
    "h5_size = 400\n",
    "h6_size = 200\n",
    "h7_size = 100\n",
    "h8_size = 50\n",
    "h9_size = 1\n",
    "\n",
    "model = tf.keras.models.Sequential(name= \"MySampleModel\")\n",
    "model.add( tf.keras.layers.InputLayer((tamano_entrada,)))\n",
    "# Here we add N layers to the model.\n",
    "model.add(tf.keras.layers.Dense(units = h0_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = h1_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = h2_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = h3_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = h4_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = h5_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = h6_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = h7_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = h8_size, activation=\"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(units = h9_size, activation=\"sigmoid\"))\n",
    "\n",
    "# Build (could also show and plot) model\n",
    "model.build()\n",
    "# print(model.summary())\n",
    "# tf.keras.utils.plot_model(model, 'MySampleModel.png')\n",
    "\n",
    "# And compile the model\n",
    "model.compile(loss=fn_perdida, optimizer=optimizador, metrics=metrica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =  500\n",
    "\n",
    "history = model.fit(x=lote_entrenamiento[0], y = lote_entrenamiento[1], batch_size = 20, epochs=num_epochs)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, precission = model.evaluate(lote_entrenamiento[0], lote_entrenamiento[1], verbose=0)\n",
    "print('\\nBaseline test accuracy: ', precission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file and save it\n",
    "keras_orig_file_path = os.path.join(os.getcwd(), 'MySampleModel.h5')\n",
    "tf.keras.models.save_model(model, keras_orig_file_path, include_optimizer=False)\n",
    "print('Saved baseline model to:', keras_orig_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (OPTIONAL) Prune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning pre-trained model with pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "batch_size = 20\n",
    "sparse_epochs = 2\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set.\n",
    "\n",
    "# num_images = tamano_lote\n",
    "end_step = np.ceil(tamano_lote/batch_size).astype(np.int32) * sparse_epochs\n",
    "\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.00,\n",
    "                                                               final_sparsity=0.95,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "# clone the model. If we do not do this, the original model will be altered too\n",
    "model_for_pruning = tf.keras.models.clone_model(model)\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model_for_pruning, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer=optimizador, # 'adam',\n",
    "              loss=fn_perdida, # tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=metrica) #['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing pruning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =  500\n",
    "\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "]\n",
    "\n",
    "history = model_for_pruning.fit(lote_entrenamiento[0], lote_entrenamiento[1], batch_size=batch_size, epochs=num_epochs, validation_split=validation_split, callbacks=callbacks)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining pruned model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pruned_precission = model_for_pruning.evaluate(lote_entrenamiento[0], lote_entrenamiento[1], verbose=0)\n",
    "print('\\nPruned model test accuracy: ', pruned_precission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results for the dense network\n",
    "temp_results = model.predict(lote_entrenamiento[0])\n",
    "# print(temp_results)\n",
    "\n",
    "print(\"\\n\\n\\n--- TOTAL DENSE ---\")\n",
    "print(sum(val >= 0.5 for val in temp_results))\n",
    "\n",
    "\n",
    "# And do the same for the pruned one\n",
    "temp_results = model_for_pruning.predict(lote_entrenamiento[0])\n",
    "# print(temp_results)\n",
    "\n",
    "print(\"\\n\\n\\n--- TOTAL PRUNED ---\")\n",
    "print(sum(val >= 0.5 for val in temp_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations before obtaining C code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/struct.html\n",
    "FLOAT_BE = \">f\"\n",
    "FLOAT_LE = \"<f\"\n",
    "DOUBLE_BE = \">d\"\n",
    "DOUBLE_LE = \"<d\"\n",
    "\n",
    "def np_value_to_hex(value, byte_format):\n",
    "    return bytearray(struct.pack(byte_format, value)).hex()\n",
    "\n",
    "# byte_format is target format for output\n",
    "def np_array_to_hex(array, byte_format):\n",
    "    return map(\n",
    "        lambda layer: list(\n",
    "            map(lambda v: np_value_to_hex(v, byte_format), layer)\n",
    "        ),\n",
    "        array,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (EXTRA) Some useful functions not used now, but helpful for debugging and development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain values for WEIGHTS and BIAS in Big-Endian (human) format, in hex. This is only useful for debugging purposes, as machines usually are LE\n",
    "for idx,layer in enumerate(model.layers,1):\n",
    "    print(f\"LAYER{idx}_WEIGHTS:\")\n",
    "    print(\"\\n\".join(\",\".join(layer) for layer in np_array_to_hex(layer.get_weights()[0], FLOAT_BE)))\n",
    "    print(f\"LAYER{idx}_BIAS:\")\n",
    "    print(\"\\n\".join(\",\".join(layer) for layer in np_array_to_hex([layer.get_weights()[1]], FLOAT_BE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export input set to text file, in order to feed it to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original stdout here, in order to print to a file\n",
    "my_stdout = sys.stdout\n",
    "\n",
    "with open('input.txt', 'w') as f:\n",
    "    sys.stdout = f\n",
    "    # Print dims to beginning of file\n",
    "    print(len(lote_entrenamiento[0]))\n",
    "    # and data to the rest of it\n",
    "    for arr in lote_entrenamiento[0]:\n",
    "        print(*arr.numpy().tolist(), sep=',')\n",
    "    # Finally restore stdout\n",
    "    sys.stdout = my_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally generate C code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense code generation\n",
    "Here we are generating C code for use with OpenBLAS. In this case, AUR package `openblas-lapack` is installed on top of a fully up-to-date Arch Linux 64 bit installation. This not only gives easy and useful library and header file integration into the system, but also allows us to fine tune compilation options, modifying `CFLAGS` and `CXXFLAGS` on `/etc/makepkg.conf`. On ubuntu `libopenblas-dev` does the trick.\n",
    "\n",
    "We need to have every file into the same folder. That means, our generated `.c` file, along with `common.{c,h}`.\n",
    "\n",
    "For compilation, we specify our desired flags in the command `gcc -march=native -O3 -s *.c -o dense.out -lm -lcblas`.\n",
    "\n",
    "If we are on Ubuntu, we need to change `-lcblas` by `-lopenblas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we finish the previous step, we need to generate the C code for compiling. This can be done by executing the code below.\n",
    "We can change `MY_MODEL` and `MAP_ALGORYTHM`, but the only one useful for changing here is `MY_MODEL`, as the latter one is the only transfer function implemented in this POC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MODEL = model\n",
    "MAP_ALGORITHM = \"sigmoid__fp32\"\n",
    "\n",
    "header = (f\"\"\"/****************************************************************************\n",
    " * Copyright (C) {date.today().year} by Alonso Rodriguez                                   *\n",
    " *                                                                          *\n",
    " * This file is an auto-generated test.                                     *\n",
    " *                                                                          *\n",
    " *   Free as in freedom.                                                    *\n",
    " ****************************************************************************/\\n\"\"\")\n",
    "\n",
    "include_libs = (\"/********************************** BEGIN INCLUDE LIBS **********************************/\\n\")\n",
    "\n",
    "for lib in [\"stdio\", \"stdlib\", \"cblas\"]:\n",
    "    include_libs += (f\"#include <{lib}.h>\\n\")\n",
    "\n",
    "for l_lib in [\"common\"]:\n",
    "    include_libs += (f\"#include \\\"{l_lib}.h\\\"\\n\")\n",
    "\n",
    "hardcoded_data = (\"/********************************** BEGIN HARD CODED DATA **********************************/\\n\")\n",
    "\n",
    "hardcoded_data += \"#define LAYER0_SIZE INPUT_SIZE\\n#define layer0_out input\\n\\n\"\n",
    "\n",
    "hardcoded_data += (f\"#define INPUT_SIZE {len(MY_MODEL.layers[0].get_weights()[0])}\\n\")\n",
    "\n",
    "idx = 0\n",
    "for layer in MY_MODEL.layers:\n",
    "    idx+=1\n",
    "\n",
    "    # Este nos da la dimensión principal 'nrows' print(len(layer.get_weights()[0]))\n",
    "    hardcoded_data += (f\"#define LAYER{idx}_SIZE {len(layer.get_weights()[0][0])}\\n\") # este nos da la secundaria 'ncols'\n",
    "\n",
    "    weight = \"\".join(\"\".join(layer) for layer in np_array_to_hex(layer.get_weights()[0], FLOAT_LE))\n",
    "    hardcoded_data += ( f\"const fp32 * layer{idx}_weights = (fp32 *) \\\"\" + r\"\\x\" + r\"\\x\".join(weight[i:i+2] for i in range(0, len(weight), 2)) + \"\\\";\\n\" )\n",
    "\n",
    "    bias = \"\".join(\"\".join(layer) for layer in np_array_to_hex([layer.get_weights()[1]], FLOAT_LE))\n",
    "    hardcoded_data += ( f\"const fp32 * layer{idx}_bias = (fp32 *) \\\"\" + r\"\\x\" + r\"\\x\".join(bias[i:i+2] for i in range(0, len(bias), 2)) + \"\\\";\\n\" )\n",
    "\n",
    "\n",
    "mallocs = (\"/**************** BEGIN MALLOCS ****************/\\n\")\n",
    "frees = (\"/**************** BEGIN FREES ****************/\\n\")\n",
    "\n",
    "# Mallocs and frees\n",
    "idx = 0\n",
    "for layer in MY_MODEL.layers:\n",
    "    idx+=1\n",
    "    mallocs += (f\"fp32 * layer{idx}_out = malloc(input_dim * LAYER{idx}_SIZE * sizeof(fp32));\\n\")\n",
    "    frees += (f\"free(layer{idx}_out);\\n\")\n",
    "\n",
    "\n",
    "algebra_ops = (\"/**************** BEGIN ALGEBRA ****************/\\n\")\n",
    "\n",
    "\n",
    "# Algebraic thinges\n",
    "idx = 0\n",
    "for layer in MY_MODEL.layers:\n",
    "    idx+=1\n",
    "\n",
    "    algebra_ops += f\"fprintf(stderr, \\\"*** SGEMM %s x %s ***\\\\n\\\", \\\"layer{idx-1}\\\", \\\"layer{idx}\\\");\\n\"\n",
    "    algebra_ops += f\"cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, input_dim, LAYER{idx}_SIZE, LAYER{idx-1}_SIZE, 1.f, (float *) layer{idx-1}_out, LAYER{idx-1}_SIZE, (float *) layer{idx}_weights, LAYER{idx}_SIZE, 0.f, (float *) layer{idx}_out, LAYER{idx}_SIZE);\\n\"\n",
    "    # Una optimización muy sencilla es meter el free de layer{idx-1} después del sgemm, pero debido a que esto es una poc, no voy a complicarme ahora mismo, a no ser que sea necesario. \n",
    "    algebra_ops += f\"fprintf(stderr, \\\"*** Map and Bias %s with %s function ***\\\\n\\\", \\\"layer{idx}_out\\\", \\\"{MAP_ALGORITHM}\\\");\\n\"\n",
    "    algebra_ops += f\"map_and_bias__fp32(layer{idx}_out, layer{idx}_bias, input_dim, LAYER{idx}_SIZE, 'N', {MAP_ALGORITHM});\\n\\n\"\n",
    "\n",
    "numlayers = len(MY_MODEL.layers)\n",
    "\n",
    "# Print from template\n",
    "\n",
    "print(f\"\"\"{header}\n",
    "\n",
    "{include_libs}\n",
    "\n",
    "{hardcoded_data}\n",
    "\"\"\"+r\"\"\"\n",
    "int main (int argc, char *argv[]) {\n",
    "\n",
    "    if(argc < 2){\n",
    "        fprintf(stderr, \"Usage: %s input_file\\n\", argv[0]);\n",
    "        exit(-1);\n",
    "    }\n",
    "\n",
    "    FILE *inputfile;\n",
    "    fprintf(stderr, \"*** Opening %s as input ***\\n\", argv[1]);\n",
    "    if((inputfile = fopen(argv[1], \"r\")) == NULL){\n",
    "        fprintf(stderr, \"  -> Error: file %s does not exist\\n\", argv[1]);\n",
    "        exit(-1);\n",
    "    }\n",
    "\n",
    "    // Parse input dims and allocate memory consquently\n",
    "    int input_dim;\n",
    "    fscanf(inputfile, \"%d\", &input_dim);\n",
    "    \n",
    "    fp32 * input;\n",
    "    input = malloc(input_dim * INPUT_SIZE * sizeof(fp32));\n",
    "\n",
    "    // Read input\n",
    "    for(int i = 0; i < INPUT_SIZE * input_dim; i++){\n",
    "        fscanf(inputfile, \"%f,\", &input[i].val);\n",
    "    }\n",
    "\n",
    "    // Finish reading input so let's close it\n",
    "    fclose(inputfile);\n",
    "\"\"\"+f\"\"\"\n",
    "    {mallocs}\n",
    "\n",
    "    {algebra_ops}\n",
    "\"\"\"+r\"\"\"\n",
    "    unsigned int greater_count = 0;\n",
    "    for(int i = 0; i < input_dim; i++){\n",
    "\"\"\"+f\"\"\"\n",
    "        if(layer{numlayers}_out[i].val >= 0.5f) greater_count += 1;\n",
    "\"\"\"+r\"\"\"\n",
    "    }\n",
    "    printf(\"\\n\\n--- SUMMARY ---\\n\");\n",
    "    printf(\" - Batch size = %d\\n\", input_dim);\n",
    "    printf(\" - Total >0.5 predictions = %d\\n\", greater_count);\n",
    "    printf(\" - %% >0.5 = %.2lf%%\\n\", ((double) greater_count / (double) input_dim)*100);\n",
    "    printf(\"\\n\");\n",
    "\"\"\"+f\"\"\"\n",
    "    {frees}\n",
    "    free(input);\n",
    "\n",
    "    return 0;\n",
    "}}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse code generation\n",
    "Here on the other hand, we are generating C code for use with LibRSB, and we only use OpenBLAS' built-in ?omatcopy for transposing the input and output matrices. Given that, on top of the already installed AUR package `openblas-lapack`, we install, also from the AUR, the `librsb` package. Again, this is done on top of a fully up-to-date Arch Linux 64 bit installation, and with already fine tuned compilation `CFLAGS` in the `makepkg.conf`. On Ubuntu, `librsb-dev` is required too, but compiling it from source with help from the PKGBUILD from the AUR package is desirable. \n",
    "\n",
    "Same building environment as before, meaning that every file is located in the same folder. (Our freshly generated `.c` file, along with `common.{c,h}`)\n",
    "\n",
    "For compilation, we re-use the previous command, while linking librsb, like so: `gcc -march=native -O3 -s *.c -o sparse.out -lm -lrsb -lcblas`.\n",
    "\n",
    "If we are on Ubuntu, here we also change `-lcblas` by `-lopenblas`.\n",
    "\n",
    "Some extra info on `librsb` compilation can be found at [their documentation webpage](http://librsb.sourceforge.net/#a_usage_c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the C code can be done with the Python code below. Same indications as the dense generation. The `MY_MODEL` constant is now particularly useful, as we can compare the computational characteristics of a fully dense implementation vs a sparse one by specifying `model_for_pruning` on the dense code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "MY_MODEL = model_for_pruning\n",
    "\n",
    "MAP_ALGORITHM = \"sigmoid__fp32\"\n",
    "\n",
    "header = (f\"\"\"/****************************************************************************\n",
    " * Copyright (C) {date.today().year} by Alonso Rodriguez                                   *\n",
    " *                                                                          *\n",
    " * This file is an auto-generated test.                                     *\n",
    " *                                                                          *\n",
    " *   Free as in freedom.                                                    *\n",
    " ****************************************************************************/\\n\"\"\")\n",
    "\n",
    "include_libs = (\"/********************************** BEGIN INCLUDE LIBS **********************************/\\n\")\n",
    "\n",
    "for lib in [\"stdio\", \"stdlib\", \"rsb\", \"blas_sparse\", \"cblas\"]:\n",
    "    include_libs += (f\"#include <{lib}.h>\\n\")\n",
    "\n",
    "for l_lib in [\"common\"]:\n",
    "    include_libs += (f\"#include \\\"{l_lib}.h\\\"\\n\")\n",
    "\n",
    "hardcoded_data = (\"/********************************** BEGIN HARD CODED DATA **********************************/\\n\")\n",
    "\n",
    "hardcoded_data += \"#define LAYER0_SIZE INPUT_SIZE\\n#define layer0_out input\\n\\n\"\n",
    "\n",
    "hardcoded_data += (f\"#define INPUT_SIZE {len(MY_MODEL.layers[0].get_weights()[0])}\\n\")\n",
    "\n",
    "for num_layer, layer in enumerate(MY_MODEL.layers,1):   \n",
    "    nonzero_count = np.count_nonzero(layer.get_weights()[0])\n",
    "    layer_num_weights = f'const fp32 * layer{num_layer}_weights = (fp32 *) \\\"'\n",
    "    layer_num_nonzero = f'#define layer{num_layer}_nz {nonzero_count}'\n",
    "    layer_num_idx_i = f'const int layer{num_layer}_i[layer{num_layer}_nz] = {{'\n",
    "    layer_num_idx_j = f'const int layer{num_layer}_j[layer{num_layer}_nz] = {{'\n",
    "\n",
    "    # Este nos da la dimensión principal 'nrows' print(len(layer.get_weights()[0]))\n",
    "    hardcoded_data += (f\"#define LAYER{num_layer}_SIZE {len(layer.get_weights()[0][0])}\\n\") # este nos da la secundaria 'ncols'\n",
    "    \n",
    "    for idx,value in enumerate(layer.get_weights()[0]):\n",
    "        for item in np.nonzero(value)[0]:\n",
    "            # print(f\"{np_value_to_hex(value[item], FLOAT_BE)}, {idx}, {item}\")\n",
    "            temp_layer_num_weight = f'{np_value_to_hex(value[item], FLOAT_LE)}'\n",
    "            layer_num_weights += r\"\\x\" + r\"\\x\".join(temp_layer_num_weight[i:i+2] for i in range(0, len(temp_layer_num_weight), 2))\n",
    "            layer_num_idx_i   += f'{idx},'\n",
    "            layer_num_idx_j   += f'{item},'\n",
    "\n",
    "    layer_num_weights += '\\\";'\n",
    "    layer_num_idx_i = layer_num_idx_i[:-1] + '};'\n",
    "    layer_num_idx_j = layer_num_idx_j[:-1] + '};'\n",
    "\n",
    "    hardcoded_data += f'{layer_num_nonzero}\\n'\n",
    "    hardcoded_data += f'{layer_num_weights}\\n'\n",
    "    hardcoded_data += f'{layer_num_idx_i}\\n'\n",
    "    hardcoded_data += f'{layer_num_idx_j}\\n'\n",
    "\n",
    "    bias = \"\".join(\"\".join(layer) for layer in np_array_to_hex([layer.get_weights()[1]], FLOAT_LE))\n",
    "    hardcoded_data += ( f\"const fp32 * layer{num_layer}_bias = (fp32 *) \\\"\" + r\"\\x\" + r\"\\x\".join(bias[i:i+2] for i in range(0, len(bias), 2)) + \"\\\";\\n\" )\n",
    "\n",
    "\n",
    "mallocs = (\"/**************** BEGIN MALLOCS ****************/\\n\")\n",
    "frees = (\"/**************** BEGIN FREES ****************/\\n\")\n",
    "matrix_creation = (\"/**************** BEGIN MATRIX CREATION ****************/\\n\")\n",
    "\n",
    "# Callocs and frees\n",
    "for idx, layer in enumerate(MY_MODEL.layers, 1):\n",
    "    mallocs += (f\"blas_sparse_matrix layer{idx}_sp_weights = blas_invalid_handle;\\n\")\n",
    "    mallocs += (f\"fp32 * layer{idx}_out = calloc(input_dim * LAYER{idx}_SIZE, sizeof(fp32));\\n\")\n",
    "    frees += (f\"BLAS_usds(layer{idx}_sp_weights);\\n\")\n",
    "    frees += (f\"free(layer{idx}_out);\\n\")\n",
    "    matrix_creation += (f\"layer{idx}_sp_weights = BLAS_suscr_begin(LAYER{idx-1}_SIZE, LAYER{idx}_SIZE);\\n\")\n",
    "    matrix_creation += (f\"BLAS_suscr_insert_entries(layer{idx}_sp_weights, layer{idx}_nz, (float *) layer{idx}_weights, layer{idx}_i, layer{idx}_j);\\n\")\n",
    "    matrix_creation += (f\"BLAS_suscr_end(layer{idx}_sp_weights);\\n\")\n",
    "\n",
    "\n",
    "algebra_ops = (\"/**************** BEGIN ALGEBRA ****************/\\n\")\n",
    "\n",
    "\n",
    "# Algebraic thinges\n",
    "for idx, layer in enumerate(MY_MODEL.layers, 1):\n",
    "    algebra_ops += f\"fprintf(stderr, \\\"*** SUSMM %s(T) x %s(T) ***\\\\n\\\", \\\"layer{idx}\\\", \\\"layer{idx-1}\\\");\\n\"\n",
    "    # algebra_ops += f\"cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, input_dim, LAYER{idx}_SIZE, LAYER{idx-1}_SIZE, 1.f, (float *) layer{idx-1}_out, LAYER{idx-1}_SIZE, (float *) layer{idx}_weights, LAYER{idx}_SIZE, 0.f, (float *) layer{idx}_out, LAYER{idx}_SIZE);\\n\"\n",
    "    algebra_ops += f\"BLAS_susmm(blas_rowmajor, blas_trans, input_dim, 1.f, layer{idx}_sp_weights, (float *) layer{idx-1}_out, input_dim, (float *) layer{idx}_out, input_dim);\\n\"\n",
    "\n",
    "    # Una optimización muy sencilla es meter el free de layer{idx-1} después del sgemm, pero debido a que esto es una poc, no voy a complicarme ahora mismo, a no ser que sea necesario. \n",
    "    algebra_ops += f\"fprintf(stderr, \\\"*** Map and Bias %s(T) with %s function ***\\\\n\\\", \\\"layer{idx}_out\\\", \\\"{MAP_ALGORITHM}\\\");\\n\"\n",
    "    algebra_ops += f\"map_and_bias__fp32(layer{idx}_out, layer{idx}_bias, LAYER{idx}_SIZE, input_dim, 'T', {MAP_ALGORITHM});\\n\\n\"\n",
    "\n",
    "\n",
    "numlayers = len(MY_MODEL.layers)\n",
    "transpose_output = (\"/*************** TRANSPOSE OUTPUT ***************/\\n\")\n",
    "transpose_output += (f\"#if LAYER{numlayers}_SIZE != 1\\n\")\n",
    "transpose_output += (f\"fp32 * temp_output = malloc(input_dim * LAYER{numlayers}_SIZE * sizeof(fp32));\\n\")\n",
    "transpose_output += (f\"fprintf(stderr, \\\"*** Transposing output out-of-place ***\\\\n\\\");\\n\")\n",
    "transpose_output += (f\"cblas_somatcopy(CblasRowMajor, CblasTrans, LAYER{numlayers}_SIZE, input_dim, 1.f, (float *) layer{numlayers}_out, input_dim, (float *) temp_output, LAYER{numlayers}_SIZE);\\n\")\n",
    "transpose_output += (f\"free(layer{numlayers}_out);\\n\")\n",
    "transpose_output += (f\"layer{numlayers}_out = temp_output;\\n\")\n",
    "transpose_output += (f\"temp_output = NULL;\\n\")\n",
    "transpose_output += (\"#endif\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Print from template\n",
    "\n",
    "print(f\"\"\"{header}\n",
    "\n",
    "{include_libs}\n",
    "\n",
    "{hardcoded_data}\n",
    "\"\"\"+r\"\"\"\n",
    "int main (int argc, char *argv[]) {\n",
    "\n",
    "    if(argc < 2){\n",
    "        fprintf(stderr, \"Usage: %s input_file\\n\", argv[0]);\n",
    "        exit(-1);\n",
    "    }\n",
    "\n",
    "    FILE *inputfile;\n",
    "    fprintf(stderr, \"*** Opening %s as input ***\\n\", argv[1]);\n",
    "    if((inputfile = fopen(argv[1], \"r\")) == NULL){\n",
    "        fprintf(stderr, \"  -> Error: file %s does not exist\\n\", argv[1]);\n",
    "        exit(-1);\n",
    "    }\n",
    "\n",
    "    // Parse input dims and allocate memory consquently\n",
    "    int input_dim;\n",
    "    fscanf(inputfile, \"%d\", &input_dim);\n",
    "    \n",
    "    fp32 * input;\n",
    "    input = malloc(input_dim * INPUT_SIZE * sizeof(fp32));\n",
    "\n",
    "    // Read input\n",
    "    for(int i = 0; i < INPUT_SIZE * input_dim; i++){\n",
    "        fscanf(inputfile, \"%f,\", &input[i].val);\n",
    "    }\n",
    "\n",
    "    // Finish reading input so let's close it\n",
    "    fclose(inputfile);\n",
    "\n",
    "    /*************** TRANSPOSE INPUT ***************/\n",
    "#if LAYER0_SIZE != 1\n",
    "    fp32 * temp_input = malloc(input_dim * INPUT_SIZE * sizeof(fp32));\n",
    "    fprintf(stderr, \"*** Transposing input out-of-place ***\\n\");\n",
    "    cblas_somatcopy(CblasRowMajor, CblasTrans, input_dim, LAYER0_SIZE, 1.f, (float *) input, LAYER0_SIZE, (float *) temp_input, input_dim);\n",
    "    free(input);\n",
    "    input = temp_input;\n",
    "    temp_input = NULL;\n",
    "#endif\n",
    "\"\"\"+f\"\"\"\n",
    "    {mallocs}\n",
    "\n",
    "    rsb_err_t errval = RSB_ERR_NO_ERROR;\n",
    "    if( (errval = rsb_lib_init(RSB_NULL_INIT_OPTIONS)) != RSB_ERR_NO_ERROR ) return -1;\n",
    "\n",
    "    {matrix_creation}\n",
    "\n",
    "    {algebra_ops}\n",
    "\n",
    "    {transpose_output}\n",
    "\"\"\"+r\"\"\"\n",
    "    unsigned int greater_count = 0;\n",
    "    for(int i = 0; i < input_dim; i++){\n",
    "\"\"\"+f\"\"\"\n",
    "        if(layer{numlayers}_out[i].val >= 0.5f) greater_count += 1;\n",
    "\"\"\"+r\"\"\"\n",
    "    }\n",
    "\n",
    "    printf(\"\\n\\n--- SUMMARY ---\\n\");\n",
    "    printf(\" - Batch size = %d\\n\", input_dim);\n",
    "    printf(\" - Total >0.5 predictions = %d\\n\", greater_count);\n",
    "    printf(\" - %% >0.5 = %.2lf%%\\n\", ((double) greater_count / (double) input_dim)*100);\n",
    "    printf(\"\\n\");\n",
    "\"\"\"+f\"\"\"\n",
    "    {frees}\n",
    "    free(input);\n",
    "\n",
    "    if( (errval = rsb_lib_exit(RSB_NULL_EXIT_OPTIONS)) != RSB_ERR_NO_ERROR ) return -1;\n",
    "\n",
    "    return 0;\n",
    "}}\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAFÍA                                                                 %
%                                                                              %
% Objetivo: Recopilar las referencias a información utilizadas y/o algunas     %
%           otras que puedan resultar interesantes                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@book{deep_learning_for_computer_architects,
author = {Reagen, Brandon and Adolf, Robert and Whatmough, Paul},
title = {Deep Learning for Computer Architects},
year = {2017},
isbn = {1627057285},
publisher = {Morgan \& Claypool Publishers},
abstract = {Machine learning, and specifically deep learning, has been hugely disruptive in many fields of computer science. The success of deep learning techniques in solving notoriously difficult classification and regression problems has resulted in their rapid adoption in solving real-world problems. The emergence of deep learning is widely attributed to a virtuous cycle whereby fundamental advancements in training deeper models were enabled by the availability of massive datasets and high-performance computer hardware. This text serves as a primer for computer architects in a new and rapidly evolving field. We review how machine learning has evolved since its inception in the 1960s and track the key developments leading up to the emergence of the powerful deep learning techniques that emerged in the last decade. Next we review representative workloads, including the most commonly used datasets and seminal networks across a variety of domains. In addition to discussing the workloads themselves, we also detail the most popular deep learning tools and show how aspiring practitioners can use the tools with the workloads to characterize and optimize DNNs. The remainder of the book is dedicated to the design and optimization of hardware and architectures for machine learning. As high-performance hardware was so instrumental in the success of machine learning becoming a practical solution, this chapter recounts a variety of optimizations proposed recently to further improve future designs. Finally, we present a review of recent research published in the area as well as a taxonomy to help readers understand how various contributions fall in context.}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={{Mathematics of Control, Signals and Systems}},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={{Neural Networks}},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@Misc{tensorflow_saved_model,
  language={english},
  title={{tf.saved\_model.load}},
  author={TensorFlow},
  url = {https://www.tensorflow.org/api_docs/python/tf/saved_model/load},
  year={2022},
  urldate = {2022-07-07}
}

@Misc{tensorflow_prune_model,
  language={english},
  title={{Pruning in Keras}},
  author={TensorFlow},
  url = {https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras},
  year={2022},
  urldate = {2022-07-07}
}

@Misc{neuralmagic_pruning_overview,
  language={english},
  title={{What is pruning in Machine Learning?}},
  author={neuralmagic.com},
  url = {https://neuralmagic.com/blog/pruning-overview/},
  year={2022},
  urldate = {2022-07-07}
}

@Misc{netlib_blas,
  language={english},
  title={{BLAS documentation on netlib}},
  author={netlib.org},
  url = {https://netlib.org/blas/},
  year={2022},
  urldate = {2022-11-02}
}

@conference{custom_high_performance_vector_codegen_sparse_computations,
author = {Horro, Marcos and Pouchet, Louis-Noël and Rodríguez, Gabriel and Touriño, Juan},
year = {2022},
title = {Custom High-Performance Vector Code Generation for Data-Specific Sparse Computations},
booktitle = {{31st International Conference on Parallel Architectures and Compilation Techniques, PACT}}
}

@thesis{horro2022manycore,
  title={{Manycore Architectures and SIMD Optimizations for High Performance Computing}},
  author={Horro, Marcos},
  type={PhD Thesis},
  institution={Universidade da Coruña},
  year={2022},
  url={http://hdl.handle.net/2183/30675}
}

@inproceedings{koppula2019eden,
  title={{EDEN: Enabling energy-efficient, high-performance deep neural network inference using approximate DRAM}},
  author={Koppula, Skanda and Orosa, Lois and Ya{\u{g}}l{\i}k{\c{c}}{\i}, A Giray and Azizi, Roknoddin and Shahroodi, Taha and Kanellopoulos, Konstantinos and Mutlu, Onur},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={166--181},
  year={2019}
}

@article{vaswani2017attention_all_you_need,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={{Advances in Neural Information Processing Systems}},
  volume={30},
  year={2017}
}

@InProceedings{chang2022maskgit,
  title = {{MaskGIT: Masked Generative Image Transformer}},
  author={Huiwen Chang and Han Zhang and Lu Jiang and Ce Liu and William T. Freeman},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2022},
  pages = {11305--11315}
}

@misc{jouppi2017_in_datacenter_tpu,
  doi = {10.48550/ARXIV.1704.04760},
  
  url = {https://arxiv.org/abs/1704.04760},
  
  author = {Jouppi, Norman P. and others},
  
  keywords = {Hardware Architecture (cs.AR), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@Misc{devopedia_tpu,
  language={english},
  title={{Tensor Processing Unit}},
  author={devopedia.org},
  url = {https://devopedia.org/tensor-processing-unit},
  year={2022},
  urldate = {2022-07-07}
}

@article{hoefler2102sparsity,
  title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks. arXiv 2021},
  author={Hoefler, T and Alistarh, D and Ben-Nun, T and Dryden, N and Peste, A},
  pages={10882--11005},
  year={2021},
  volume={22},
  number={1},
  journal={The Journal of Machine Learning Research}
}

@article{sparse_blas_10.1145/567806.567810,
author = {Duff, Iain S. and Heroux, Michael A. and Pozo, Roldan},
title = {{An Overview of the Sparse Basic Linear Algebra Subprograms: The New Standard from the BLAS Technical Forum}},
year = {2002},
issue_date = {June 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/567806.567810},
doi = {10.1145/567806.567810},
abstract = {We discuss the interface design for the Sparse Basic Linear Algebra Subprograms (BLAS), the kernels in the recent standard from the BLAS Technical Forum that are concerned with unstructured sparse matrices. The motivation for such a standard is to encourage portable programming while allowing for library-specific optimizations. In particular, we show how this interface can shield one from concern over the specific storage scheme for the sparse matrix. This design makes it easy to add further functionality to the sparse BLAS in the future.We illustrate the use of the Sparse BLAS with examples in the three supported programming languages, Fortran 95, Fortran 77, and C.},
journal = {ACM Trans. Math. Softw.},
month = {jun},
pages = {239-267},
numpages = {29},
keywords = {sparse BLAS, software, Algorithms, computational kernels, sparse matrices, sparse iterative methods}
}
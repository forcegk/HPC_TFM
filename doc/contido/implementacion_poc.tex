\chapter{Desarrollo y Prueba de Concepto}
\label{chap:desarrollo_poc}

\lettrine{T}{ras} identificar los puntos de intervención y posible mejora, así como habiendo revisado los trabajos ya realizados, en este capítulo se detalla el desarrollo e implementación de una Prueba de Concepto que sirva para generar diferentes códigos C para una misma red neuronal, creada y entrenada con una librería real. Los conceptos de codificación necesarios para la creación de dichos ficheros C, incluyendo las llamadas a librerías, paralelización, etc. se explican de forma incremental y según la arquitectura de la red. Finalmente se discuten otras implementaciones derivadas y sus posibilidades.

Los códigos desarrollados a lo largo de este capítulo, así como el \textit{Notebook}, se pueden encontrar en la raíz del proyecto\footnote{\url{https://github.com/forcegk/HPC_TFM}} bajo las carpetas \texttt{res/} para ficheros de TensorFlow y código C, así como en \texttt{src/\{c,python\}} para el generador y ficheros \texttt{common.\{c,h\}}.

\section{Objetivos de la Prueba de Concepto}
\label{sec:objetivos_poc}
El objetivo de esta Prueba de Concepto es implementar una red neuronal para la posterior medida de su rendimiento y perfilado. Sin embargo, dado que uno de los \textit{frameworks} predominantes en la industria es TensorFlow, tal como se comenta en la Sección \ref{sec:tensorflow}, se ha decidido desarrollar las redes neuronales mediante este \textit{framework}, ahorrando toda la implementación del entrenamiento (que no concierne a este trabajo). Esto permite no solo investigar acerca de la implementación a bajo nivel de esta librería, sino además comparar los resultados que arroja la implementación de la \acrshort{poc} con los de dicha librería, verificando así que el programa sea correcto. Por último, el uso de TensorFlow también es necesario para el podado de redes neuronales, donde es importante que la forma de las matrices sea algo extrapolable a una red neuronal real.

Para cumplir estos objetivos se ha creado un \textit{Jupyter Notebook} en el cual se puede ir ejecutando paso a paso cada etapa en la generación y entrenamiento de la red, así como opcionalmente crear una variante podada del modelo, para finalmente generar código C. Los pasos para la ejecución del programa, así como los conocimientos necesarios para comprender las necesidades del mismo, se describen a continuación.

A pesar de que el perfilado de un código \textit{machine learning} y la extracción de sus características no precisa de ningún tipo de generación dinámica de código C, se ha decidido realizar esto y no una simple medida de rendimiento en el propio TensorFlow, con el fin de mejorar los conocimientos y comprensión de estas redes a bajo nivel. Aunque esto pueda mermar la cantidad de arquitecturas de red diferentes que se pueden medir, resulta particularmente útil a la hora de continuar la investigación realizada en redes con arquitectura \textit{point-to-point} (ver Subsección \ref{ssec:gdin_arquitectura_point_to_point}). 

\section{Extracción de valores para modelos TensorFlow}
\label{sec:extraccion_valores_modelo_tf}
El primer paso para la implementación de esta \acrshort{poc} es la extracción de los valores desde el modelo TensorFlow. Este modelo puede venir dado previamente en formato \texttt{.pb} (que define la arquitectura de red, así como los valores de los pesos, \textit{bias}, etc.), o puede ser creado y entrenado directamente desde Python sin importarlo o exportarlo en ningún momento.

Dado que los modelos más grandes y complejos, como los que se pueden encontrar en la \textit{ONNX Model Zoo} (ver Sección \ref{sec:tensorflow}), no suelen ser modelos exclusivamente \textit{feed-forward} con funciones de activación sigmoides, lo más cómodo es simplemente generar una red neuronal de pruebas, entrenarla, y exportar el resultado a C.

\subsection{Desde un modelo guardado}
\label{ssec:desde_modelo_guardado}
Para realizar esto, que queda fuera del objetivo de este trabajo, bastaría con emplear la función \texttt{saved\_model.load} de TensorFlow sobre un modelo \texttt{.pb} previamente exportado \cite{tensorflow_saved_model}. Esto permitiría importar grandes modelos preentrenados y realizar mediciones sobre modelos ``del mundo real'', pero implicaría una gran cantidad de horas de trabajo y adaptación, debido a la gran cantidad de parámetros, tipos de capas y funciones de activación, entre otros, que no se han implementado.

Por esta razón, este proyecto se limita a la extracción para modelos puramente \textit{feed-forward} con función de activación sigmoide (a pesar de que añadir soporte para distintas funciones de activación sería relativamente sencillo de implementar).

\subsection{Desde un modelo de pruebas}
\label{ssec:desde_modelo_de_pruebas}
Este es el camino más sencillo a la hora de implementar lo que, debe recordarse, es simplemente un laboratorio básico para poder realizar mediciones y extraer conclusiones.

\subsubsection{Fase común}
\label{sssec:modelo_pruebas_fase_comun}
A continuación se muestran, por orden de aparición en el \textit{Notebook}, los pasos de entrenamiento de la red:

\begin{enumerate}
    \item Se importa el conjunto de datos. En este caso se ha optado por el ampliamente utilizado \textit{dataset} \texttt{german\_credit\_numeric}, debido a la simplicidad de sus entradas y salidas, así como por los numerosos ejemplos disponibles para este. El \textit{dataset} cuenta con 1000 datos de entrada de 24 parámetros numéricos cada uno, y se emplea para intentar evaluar el riesgo crediticio de un individuo mediante redes neuronales:\medskip
\begin{lstlisting}[language=Python]
ds = tfds.load('german_credit_numeric', split='train', as_supervised=True)
\end{lstlisting}

    \item Se indica la función de pérdida, el optimizador y la métrica. Esto sería un tema muy interesante si este trabajo tratase de obtener buenos resultados con esta red; sin embargo en este caso es irrelevante que la arquitectura de la red sea buena o no, así como la calidad de su entrenamiento y resultados.

    \item Se crea un modelo secuencial, es decir, un modelo ``capa por capa'', en el que cada capa va a continuación de la siguiente. Tras crearlo se \textit{buildea} (\texttt{model.build()}) y se compila:\medskip
\begin{lstlisting}[language=Python]
model = tf.keras.models.Sequential(name= "MySampleModel")
model.add(tf.keras.layers.InputLayer((tamano_entrada,)))
model.add(tf.keras.layers.Dense(units = h0_size, activation="sigmoid"))
model.add(tf.keras.layers.Dense(units = h1_size, activation="sigmoid"))
model.add(tf.keras.layers.Dense(units = 1, activation="sigmoid"))

model.build()

model.compile(loss=fn_perdida, optimizer=optimizador, metrics=metrica)
\end{lstlisting}

    \item Tras la creación del modelo hay que entrenarlo. Este complejo proceso a nivel de implementación, basado en el algoritmo de \textit{backpropagation}, y con todos los detalles que este implica, se puede realizar en, al más puro estilo Python, con un \textit{one-liner}:\medskip
\begin{lstlisting}[language=Python]
history =  model.fit(x=lote_entrenamiento[0], y = lote_entrenamiento[1], batch_size = 20, epochs=num_epochs)
\end{lstlisting}
\end{enumerate}

\subsubsection{Fase de podado}
\label{sssec:modelo_pruebas_fase_podado}
En caso de querer podar la red para aumentar su \textit{sparsity}, el procedimiento es muy similar al descrito en \cite{tensorflow_prune_model}. Los pasos para realizar este procedimiento se describen a continuación:
\begin{enumerate}
    \item Se crean los parámetros para podar el modelo hasta una \textit{sparsity} deseada, en este caso del 80\%. Previamente al podado se copia el modelo original a un modelo diferente, \texttt{model\_for\_pruning}:\medskip
\begin{lstlisting}[language=Python]
prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude

batch_size = 20
sparse_epochs = 2
validation_split = 0.1

end_step = np.ceil(tamano_lote/batch_size).astype(np.int32) * sparse_epochs

pruning_params = {
    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(
        initial_sparsity=0,                                                    
        final_sparsity=0.8,
        begin_step=0,
        end_step=end_step
    )
}

# clone the model. If we do not do this, the original model will be altered too
model_for_pruning = tf.keras.models.clone_model(model)

model_for_pruning = prune_low_magnitude(model_for_pruning, **pruning_params)

# `prune_low_magnitude` requires a recompile.
model_for_pruning.compile(optimizer=optimizador, # 'adam',
                loss=fn_perdida,
                metrics=metrica)

model_for_pruning.summary()
\end{lstlisting}
    \item Se entrena el modelo podado. Este procedimiento es muy similar al entrenamiento en una línea explicado previamente, únicamente añadiendo el \textit{callback} necesario para el entrenamiento de un modelo podado:\medskip
\begin{lstlisting}[language=Python]
num_epochs =  500

callbacks = [
    tfmot.sparsity.keras.UpdatePruningStep(),
]

history = model_for_pruning.fit(lote_entrenamiento[0], lote_entrenamiento[1], batch_size=batch_size, epochs=num_epochs, validation_split=validation_split, callbacks=callbacks)
history.history
\end{lstlisting}
\end{enumerate}

\subsection{Extracción de valores}
\label{ssec:extraccion_valores}
Ahora que se cuenta con un modelo de red neuronal de discutible calidad, pero funcionalmente idéntico a otro mejor adaptado al problema a resolver, se puede comenzar con la extracción de valores de cada capa.

\begin{enumerate}
    \item Primero es necesario obtener tanto los pesos como los \textit{bias}. Para esto se itera sobre las capas del modelo. En cada capa del modelo se puede llamar a la función \texttt{get\_weights()}. En el resultado que devuelve, los pesos se encuentran en un array de arrays en la posición 0, y los \textit{bias} en otro array, en la posición 1:\medskip
\begin{lstlisting}[language=Python]
for layer in model.layers:
    print(f"LAYER{idx}_WEIGHTS:")
    print(layer.get_weights()[0])
    print(f"LAYER{idx}_BIAS:")
    print([layer.get_weights()[1]])

# LAYER1_WEIGHTS:
# [[-0.08928536  0.08118167 -1.6078503  -0.29092655 -0.33163825]
#                               ...
#  [-0.16627118 -0.36254594  0.03569769  0.11377215 -0.19968267]]
# LAYER1_BIAS:
# [ 0.00605693  0.00038876  0.29822487  0.01296887 -0.02708104]
# LAYER2_WEIGHTS:
# [[-0.43815556  0.6910728   0.45901924]
#                   ...
#  [ 1.3233485  -0.26594874 -0.5277702 ]]
# LAYER2_BIAS:
# [-0.1541544  -0.04604432  0.15921216]
# LAYER3_WEIGHTS:
# [[-3.4663067 ]
#       ...
#  [ 1.5324626 ]]
# LAYER3_BIAS:
# [1.3300072]
\end{lstlisting}
    Sin embargo, esta aproximación es problemática a la hora de obtener los pesos. Y es que al mostrar los pesos en formato humano (decimal), se pierde precisión. Para solucionar esto se necesita algo más directo y de bajo nivel.
    
    \item La solución a la que se ha llegado es la más evidente, exportar los datos en formato máquina, es decir, binario codificado en hexadecimal. Realizar esto en Python no es particularmente difícil, y si bien existen mejores formas de ``interfacear'' con código C, conviene recordar que esto es simplemente una prueba de laboratorio y no un producto final. De esta forma, se define una función para la conversión de datos en hexadecimal:\medskip
\begin{lstlisting}[language=Python]
# https://docs.python.org/3/library/struct.html
FLOAT_BE = ">f"
FLOAT_LE = "<f"
DOUBLE_BE = ">d"
DOUBLE_LE = "<d"

def np_value_to_hex(value, byte_format):
    return bytearray(struct.pack(byte_format, value)).hex()

# byte_format is target format for output
def np_array_to_hex(array, byte_format):
    return map(
        lambda layer: list(
            map(lambda v: np_value_to_hex(v, byte_format), layer)
        ),
        array,
    )
\end{lstlisting}
    Y tras crear dicha función, ahora con un poco de manejo de strings, fácilmente se puede obtener un resultado como el siguiente:\medskip
\begin{lstlisting}[language=Python]
# LAYER1_WEIGHTS:
# bdb6db3e,3da64293,bfcdce0a,be94f453,bea9cc7d
#                     ...
# be2a42fe,beb99f9f,3d1237be,3de90160,be4c799d
# LAYER1_BIAS:
# 3bc67940,39cbd218,3e98b0ee,3c547b67,bcddd911
# LAYER2_WEIGHTS:
# bee055ed,3f30ea26,3eeb0492
#            ...
# 3fa9637c,be882a6f,bf071bf3
# LAYER2_BIAS:
# be1ddaa7,bd3c98f7,3e230883
# LAYER3_WEIGHTS:
# c05dd7f8
#   ...
# 3fc427bc
# LAYER3_BIAS:
# 3faa3dad
\end{lstlisting}

    \item Por último, pero no por ello menos importante, se han de exportar también los datos de entrada del propio \textit{dataset}. Como se discutirá más adelante, el formato del documento que lee el código C compilado contará con una primera línea especificando el número de líneas que va a leer, y un dato por línea. Así, si la entrada contiene un total de 1000 datos para inferencia, el contenido del documento será de 1001 líneas, siendo la primera \texttt{1000}, y el resto los datos multidimensionales a inferir:\medskip
\begin{lstlisting}[language=Python]
# Get original stdout here, in order to print to a file
my_stdout = sys.stdout

with open('input.txt', 'w') as f:
    sys.stdout = f
    # Print dims to beginning of file
    print(len(lote_entrenamiento[0]))
    # and data to the rest of it
    for arr in lote_entrenamiento[0]:
        print(*arr.numpy().tolist(), sep=',')
    # Finally restore stdout
    sys.stdout = my_stdout
\end{lstlisting}
    Al ejecutar el código anterior, se crea en el directorio de trabajo el fichero \texttt{input.txt}, que contiene un total de 1000 entradas en este caso. Para este ejemplo, el fichero se ha recortado a 5 entradas por motivos de espacio:\medskip
\begin{lstlisting}
5
3,6,4,13,2,5,1,4,3,28,3,2,2,2,1,1,0,1,0,0,1,0,0,1
4,4,2,6,1,2,2,3,1,23,3,1,2,1,1,0,0,1,0,1,0,0,1,0
4,24,4,20,1,3,2,4,3,37,3,1,1,2,1,1,0,1,0,0,1,0,0,1
4,18,2,11,5,2,2,2,1,21,3,1,1,2,1,0,0,1,0,1,0,0,0,1
4,6,2,13,3,3,1,4,1,62,3,1,1,1,1,0,0,1,0,0,1,0,0,1
\end{lstlisting}

    Conviene recalcar que en este caso el tipo de los datos de entrada es entero. En caso de ser datos en coma flotante, se procedería, en función de la relevancia de la precisión de los datos, a exportar en formato decimal (precisión poco relevante) o hexadecimal (precisión muy relevante).
\end{enumerate}

\subsubsection{Extracción de datos dispersos}
\label{sssec:extraccion_datos_dispersos}
Para la extracción de datos dispersos se procede de igual forma que con datos densos. Iterando a través de cada una de las capas con funciones como las expuestas previamente, se obtienen resultados como el siguiente:\medskip
\begin{lstlisting}[language=Python]
# LAYER1_WEIGHTS:
# 80000000,80000000,00000000,00000000,00000000
#                     ...
# 3ec9393d,80000000,80000000,00000000,00000000
# LAYER1_BIAS:
# 39fcfbe4,3854f5e8,b82401d6,b8b52b95,ba28adf7
# LAYER2_WEIGHTS:
# 80000000,00000000,80000000
#            ...
# 80000000,80000000,3f5a98cf
# LAYER2_BIAS:
# 3bea3c98,3d964016,3b89974c
# LAYER3_WEIGHTS:
# 80000000
#   ...
# 3f61fe09
# LAYER3_BIAS:
# 3ec81415    
\end{lstlisting}

Como se puede apreciar dentro de las zonas recortadas, la mayor parte de datos son 0x80000000 o 0x00000000, siendo ambos representaciones del número cero en coma flotante. Con la ayuda de la función \texttt{nonzero} de \texttt{numpy} se puede extraer fácilmente cada valor diferente de cero con sus coordenadas verticales y horizontales para importar más adelante en C, tal como se muestra en la siguiente sección.

\section{Importación de valores en C}
\label{sec:importacion_valores_c}
En esta sección se describe cómo se importan los datos extraídos de los modelos TensorFlow densos o dispersos, creados en la sección anterior.

\subsection{Importación de matrices densas}
\label{ssec:importacion_matrices_densas}
El lenguaje de programación C almacena las matrices por filas. Aprovechando esta característica, y haciendo uso del tan granular acceso a bajo nivel que nos permite hacer este lenguaje, se importan las matrices de pesos y \textit{bias} desde la sección \texttt{.text}, debido a que son cadenas de solo lectura. Esto implica que, una vez que el programa se carga en memoria, este no lee ningún archivo salvo la entrada de datos.

Mediante funciones de manejo de \textit{strings} en Python para la generación de estas líneas de código, y recordando que la arquitectura x86(\_64) es \textit{little endian}\footnote{\url{https://es.wikipedia.org/wiki/Endianness}}, los datos de cada una de las capas se importan al código con las siguientes líneas:\medskip
\begin{lstlisting}[language=C]
#define INPUT_SIZE 24
#define LAYER1_SIZE 5
const fp32 * layer1_weights = (fp32 *)
        "\xf3\x79\x85 /* [...] */ \x8c\xab\xbe";
const fp32 * layer1_bias = (fp32 *)
        "\x3f\x1b\x5f /* [...] */ \x54\x37\x3c";
#define LAYER2_SIZE 3
const fp32 * layer2_weights = (fp32 *)
        "\x16\xbd\xc5 /* [...] */ \x8d\x77\x3f";
const fp32 * layer2_bias = (fp32 *)
        "\xfc\x06\x25 /* [...] */ \xa4\x2a\xbe";
#define LAYER3_SIZE 1
const fp32 * layer3_weights = (fp32 *)
        "\x09\x88\x1d /* [...] */ \x57\x04\x40";
const fp32 * layer3_bias = (fp32 *) "\xc8\x2d\x40\x3d";
\end{lstlisting}

Estos valores, al ser \textit{casteados} a un vector de \texttt{fp32}, pueden ser accedidos más adelante con el \textit{stride} de dicho tipo de dato (en este caso, 32 bits).

\subsection{Importación de matrices dispersas}
\label{ssec:importacion_matrices_dispersas}
En el caso de las matrices dispersas, el almacenamiento de datos no es tan sencillo. Como se comentó en el Capítulo \ref{chap:conceptos_basicos}, existen múltiples formatos de representación de matrices dispersas en memoria. El formato que se emplea en este caso es el \acrshort{coo}, aprovechando así las facilidades que ofrece esta representación en cuanto a programabilidad (ver Subsección \ref{ssec:almacenamiento_matrices_dispersas}). Debido a la elección de este formato, es suficiente con importar en el código C una lista de valores, acompañada de las coordenadas de cada uno de ellos. Estos valores, tanto los de la matriz como las coordenadas de los mismos, de la misma forma en la que se hizo al importar matrices densas, se guardan en la sección \texttt{.text}.

Esto significa que, en comparación con la versión densa, el vector de pesos es mucho más pequeño (en función de la densidad de la matriz que representa), y aparecen dos vectores de coordenadas, que identifican las abscisas y ordenadas de cada dato \textit{nonzero}, siendo estos \texttt{i} y \texttt{j}, respectivamente:\medskip
\begin{lstlisting}[language=C]
#define INPUT_SIZE 24
#define LAYER1_SIZE 5
#define layer1_nz 24
const fp32 * layer1_weights = (fp32 *)
        "\x7c\xce\xbc /* [...] */ \x39\xc9\x3e";
const int layer1_i[layer1_nz] =
        {1,1,2,3,3,4,8,11,12,12,13,14,15,15,15,17,18,18,19,19,20,21,21,23};
const int layer1_j[layer1_nz] =
        {0,1,1,1,2,0,3,2,1,2,1,4,1,2,4,3,1,2,0,1,2,3,4,0};
const fp32 * layer1_bias = (fp32 *)
        "\xe4\xfb\xfc /* [...] */ \xad\x28\xba";
#define LAYER2_SIZE 3
#define layer2_nz 3
const fp32 * layer2_weights = (fp32 *)
        "\xe2\x81\x50 /* [...] */ \x98\x5a\x3f";
const int layer2_i[layer2_nz] = {2,3,3};
const int layer2_j[layer2_nz] = {2,0,2};
const fp32 * layer2_bias = (fp32 *)
        "\x98\x3c\xea /* [...] */ \x97\x89\x3b";
#define LAYER3_SIZE 1
#define layer3_nz 1
const fp32 * layer3_weights = (fp32 *) "\x09\xfe\x61\x3f";
const int layer3_i[layer3_nz] = {1};
const int layer3_j[layer3_nz] = {0};
const fp32 * layer3_bias = (fp32 *) "\x15\x14\xc8\x3e";
\end{lstlisting}


\section{Generación dinámica de código C a partir del modelo TensorFlow}
\label{sec:generacion_din_modelo_tf}
Tras ver cómo extraer los valores de pesos, \textit{bias} y datos de entrada del entorno de TensorFlow, se cuenta con todas las herramientas necesarias para generar el código C, o código para prácticamente cualquier lenguaje si se cuenta con los conocimientos adecuados.

\subsection{Decisiones previas de diseño}
\label{ssec:decisiones_previas_diseno}
Las funciones, definiciones de tipo, y otros recursos comunes a todos los códigos generados, sean estos densos o dispersos, se han centralizado en los ficheros \texttt{common.\{c|h\}}. Primeramente, se ha decidido emplear un tipo \texttt{union} para la implementación de los datos en coma flotante, al permitir estos un control superior a nivel de byte, sin complicaciones adicionales como \textit{casting} de punteros. Los únicos dos tipos de datos implementados son \texttt{fp32} y \texttt{fp64}, con nombres representativos de su precisión. Por ejemplo, el tipo \texttt{union} para el flotante de 32 bits es el siguiente:\medskip
\begin{lstlisting}[language=C]
typedef union _fp32 {
    uint32_t raw;
    float val;
    uint8_t byte[4];
} fp32;
\end{lstlisting}

Así, las funciones implementadas tendrán versiones para flotantes de 32 y 64 bits. El sufijo que indica la precisión del flotante será \texttt{\_\_fp32} y \texttt{\_\_fp64} para 32 y 64 bits de precisión, respectivamente. Las pocas funciones implementadas han sido necesarias para la aplicación del \textit{bias}, y siguen esta nomenclatura con el propósito de mejorar una futura parametrización extra en la generación de código.

Se ha barajado también la posibilidad de emplear funciones genéricas que brinda el compilador GCC, pero al final, por simplicidad, y dada la naturaleza estática de una Prueba de Concepto, se ha optado por no incluir estas macros. Un ejemplo para la función \texttt{fast\_sigmoid} sería el siguiente:\medskip
\begin{lstlisting}[language=C]
#define fast_sigmoid(X) _Generic((X), \
fp32:fast_sigmoid__fp32, \
fp64:fast_sigmoid__fp64 \
)(X)

fp32 fast_sigmoid__fp32(fp32);
fp64 fast_sigmoid__fp64(fp64);
\end{lstlisting}


\subsection{Matrices densas}
\label{ssec:gdin_matrices_densas}
La forma más sencilla (y cronológicamente la primera en implementarse) es la basada en matrices densas. Esta implementación se apoya en la ampliamente disponible librería \texttt{OpenBLAS}\footnote{\url{https://www.openblas.net}}. A pesar de que el proceso de desarrollo, pruebas, automatización y corrección de posibles bugs pueda ser algo tedioso, en realidad la teoría es moderadamente simple.

Tal como se comenta en la Sección \ref{sec:redes_reuronales_densas}, el programa debe hacer lo siguiente:

\begin{enumerate}
    \item Leer los datos de entrada.
    \item Multiplicar los datos de entrada por cada una de las capas.
    \item Tratar adecuadamente los datos de salida.
\end{enumerate}

Estos tres sencillos pasos tienen su complicación, debido a que hay que implementar todo desde cero y automatizar su generación. Para cada punto anterior, la estructura básica del programa debe ser la siguiente:

\begin{enumerate}
    \item Leer los datos de entrada es quizás de lo más sencillo. Es manejo de ficheros básico mediante \texttt{fscanf}:\medskip
\begin{lstlisting}[language=C]
FILE *inputfile;
fprintf(stderr, "*** Opening %s as input ***\n", argv[1]);
if((inputfile = fopen(argv[1], "r")) == NULL){
    fprintf(stderr, "  -> Error: file %s does not exist\n", argv[1]);
    exit(-1);
}

// Parse input dims and allocate memory consequently
int input_dim;
fscanf(inputfile, "%d", &input_dim);

fp32 * input;
input = malloc(input_dim * INPUT_SIZE * sizeof(fp32));

// Read input
for(int i = 0; i < INPUT_SIZE * input_dim; i++){
    fscanf(inputfile, "%f,", &input[i].val);
}

// Finish reading input so let's close it
fclose(inputfile);
\end{lstlisting}

    \item Para realizar la multiplicación de matrices, y siguiendo con este ejemplo, se emplea la función \texttt{cblas\_sgemm}. Esta función realiza un producto de matrices ampliado tal que $C = \alpha AB + \beta C$. El resultado se guarda de forma aditiva sobre $C$, por lo que es necesario que o bien dicha matriz esté a cero, o el factor $\beta$ sea cero:\medskip
\begin{lstlisting}[language=C]
fprintf(stderr, "*** SGEMM %s x %s ***\n", "layer0", "layer1");
cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, input_dim,
            LAYER1_SIZE, LAYER0_SIZE, 1.f, (float *) layer0_out,
            LAYER0_SIZE, (float *) layer1_weights, LAYER1_SIZE,
            0.f, (float *) layer1_out, LAYER1_SIZE);
\end{lstlisting}

    Tras la multiplicación de $A$ por $B$, aditiva sobre $C$, se han de aplicar los \textit{bias} y la función de transferencia. Esto se realiza con una función \textit{ad hoc} que combina ambas acciones en un solo recorrido lineal por memoria, \texttt{map\_and\_bias}:\medskip
\begin{lstlisting}[language=C]
fprintf(stderr, "*** Map and Bias %s with %s function ***\n", "layer1_out", "sigmoid__fp32");
map_and_bias__fp32(layer1_out, layer1_bias, input_dim, LAYER1_SIZE, 'N', sigmoid__fp32);
\end{lstlisting}

    Si bien es cierto que en lugar de aplicar los \textit{bias} junto a la función de transferencia en la función previa se puede realizar $C = AB + bias$ (con una copia de \textit{bias} por fila), es mejor realizar esta operación en conjunción con la aplicación de la función de transferencia, ya que así se obtiene una mayor intensidad aritmética. Esta mayor intensidad es especialmente relevante cuando se paraleliza la función, ya que lo más probable es que \texttt{map\_and\_bias} sea \textit{memory-bound} en mayor o menor medida según la función de transferencia. Además, si se operase sobre una matriz pre-inicializada con los \textit{bias} en cada fila, se necesitaría replicar los valores de \textit{bias} \texttt{input\_dim}\footnote{\texttt{input\_dim} es el número de filas de la entrada, es decir, el número de datos multidimensionales que entran a la red.} veces, replicación que no es gratuita, y que sería indudablemente \textit{memory-bound}. Por estas razones es más conveniente fusionar los bucles en uno solo para optimizar los accesos a memoria principal.

    Una implementación básica de \texttt{map\_and\_bias} es similar a la siguiente:\medskip
\begin{lstlisting}[language=C]
void map_and_bias__fp32(fp32 *restrict A, const fp32 *restrict bias, const uint32_t M, const uint32_t N, fp32 (* map_function)(fp32 x)){
    for (uint32_t i = 0; i < M; i++){
        for(uint32_t j = 0; j < N; j++){
            A[i*N+j].val = map_function((fp32)(A[i*N+j].val +
                                        bias[j].val)).val;
        }
    }
}
\end{lstlisting}

    \item Tras repetir el paso anterior tantas veces como capas tenga la arquitectura, la salida de cada capa \texttt{X} de la red se puede encontrar en la matriz \texttt{layerX\_out}. Debido a esto, para una red neuronal de \texttt{N} capas se puede encontrar la salida de la red en la matriz \texttt{layerN\_out}. Estos datos pueden ser exportados, mostrados, contados, posprocesados, etc. Como en este caso interesa perfilar y visualizar las características del \textit{workload}, lo más lógico es primero probar con una cantidad reducida de datos para verificar el correcto funcionamiento del código generado.
    
    Tras verificar el correcto funcionamiento de la red, y observar que para la implementación con \texttt{OpenBLAS} los resultados son exactamente iguales a los proporcionados por \texttt{numpy}, se añade un pequeño resumen de ejecución, que clasifica el número de predicciones mayores o menores a 0,5. Esto es realmente útil para comparar la validez de los resultados con respecto a TensorFlow de forma rápida:\medskip
\begin{lstlisting}[language=C]
printf("\n\n--- SUMMARY ---\n");
printf(" - Batch size = %d\n", input_dim);
printf(" - Total >0.5 predictions = %d\n", greater_count);
printf(" - %% >0.5 = %.2lf%%\n", ((double) greater_count / (double) input_dim)*100);
printf("\n");
\end{lstlisting}

\end{enumerate}


\subsection{Matrices dispersas}
\label{ssec:gdin_matrices_dispersas}
Una vez detallado cómo se procede con matrices de pesos densas, y según lo visto en la Sección \ref{sec:redes_reuronales_dispersas}, los pasos a mayores que se deben realizar para implementar una red neuronal basada en matrices dispersas son:

\begin{enumerate}
    \item Transponer la entrada y salida.
    \item Ajustar los parámetros de las funciones densas.
    \item Sustituir las matrices de pesos densas por dispersas.
    \item Sustituir las funciones de multiplicación de matrices densas por sus equivalentes dispersas.
\end{enumerate}

Para realizar estos pasos primero se necesita una librería \acrshort{blas} que soporte matrices dispersas y que exponga una cabecera para C. A ser posible, y como en cualquier proyecto, es conveniente que una dependencia \textit{core}, como lo es la librería de multiplicación de matrices en este caso, sea moderadamente conocida, dado que ello asegurará una mayor calidad del código interno, así como un mejor soporte. Cumpliendo estos requerimientos, se ha optado por la utilización de la librería \texttt{librsb}\footnote{Página principal \url{http://librsb.sourceforge.net}, con paquete en el AUR (ArchLinux User Repository), \url{https://aur.archlinux.org/packages/librsb}, así como paquete mantenido en Ubuntu, si bien desactualizado en \url{https://packages.ubuntu.com/jammy/librsb-dev}}, de la cual se emplea su interfaz Sparse BLAS. De esta forma, los pasos para adaptar el código para trabajar con matrices de pesos dispersas son:

\begin{enumerate}
    \item Para transponer las matrices de entrada y de salida se emplea una función \textit{built-in} de \texttt{OpenBLAS}, por lo que en este aspecto el código está ligado a dicha librería. De todos modos, no parece una dependencia muy complicada de sobrellevar, puesto que esta implementación es muy habitual en la industria, y además es una función que solo se usará en esta Prueba de Concepto. A continuación se muestra cómo transponer la entrada (para transponer la salida se procede de forma análoga, como se indica más adelante):\medskip
\begin{lstlisting}[language=C]
fprintf(stderr, "*** Transposing input out-of-place ***\n");
cblas_somatcopy(CblasRowMajor, CblasTrans, input_dim, LAYER0_SIZE, 1.f, (float *) input, LAYER0_SIZE, (float *) temp_input, input_dim);
\end{lstlisting}

    Este procedimiento, que se realiza \textit{out-of-place} por la mejora de rendimiento que supone al transponer matrices no cuadradas, se complementa con lógica básica de punteros previa y posterior a la llamada a función, en la que no es necesario profundizar, pero que hace que esta transposición sea transparente al resto del código. A continuación se muestra como ejemplo el bloque que realiza la transposición de la capa de salida de forma transparente al resto del programa, para una red neuronal de tres capas:\medskip
\begin{lstlisting}[language=C]
#if LAYER3_SIZE != 1
fp32 * temp_output = malloc(input_dim * LAYER3_SIZE * sizeof(fp32));
fprintf(stderr, "*** Transposing output out-of-place ***\n");
cblas_somatcopy(CblasRowMajor, CblasTrans, LAYER3_SIZE, input_dim, 1.f, (float *) layer3_out, input_dim, (float *) temp_output, LAYER3_SIZE);
free(layer3_out);
layer3_out = temp_output;
temp_output = NULL;
#endif
\end{lstlisting}

    \item En cuanto a ``ajustar los parámetros de las funciones densas'', este es un paso intermedio que no se aprecia en la versión final. En este paso se comprueba que el resultado no varía al adaptar el código a las funciones Sparse \acrshort{blas} de \texttt{librsb}. Fuera de los cambios en los parámetros, también se hace necesario un cambio en la función \texttt{map\_and\_bias}, que ahora debe aplicar los \textit{bias} en un \textit{layout} de memoria diferente. Para esto se añade un parámetro \texttt{transA}. Este parámetro no transpone la matriz, como sí que ocurre en \acrshort{blas}, sino que indica a \texttt{map\_and\_bias} que la matriz sobre la que aplica los \textit{bias} y la función de transferencia está transpuesta en memoria, por lo que el \textit{traversing} debe ser diferente. La modificación de dicha función se muestra, simplificada, en el siguiente código:\medskip
\begin{lstlisting}[language=C]
void map_and_bias__fp32(fp32 *restrict A, const fp32 *restrict bias, const uint32_t M, const uint32_t N, const char transA, fp32 (* map_function)(fp32 x)){
    switch (transA){
    case 'N':
    case 'n':
        // [...] algoritmo inicial
        break;

    case 'T':
    case 't':
        for (uint32_t i = 0; i < M; i++){
            for(uint32_t j = 0; j < N; j++){
                A[i*N+j].val = map_function((fp32)(A[i*N+j].val 
                                            + bias[i].val)).val;
            }                                 // ^^^^^
        }                    // aquí se cambia el orden de acceso a bias
        break;
    }
}
\end{lstlisting}

    \item El siguiente paso es sustituir las matrices de pesos densas por matrices de pesos dispersas. Para esto se emplea el mismo método de \textit{hardcodeo} de datos del código de la Subsección \ref{ssec:importacion_matrices_dispersas} mediante el uso de \texttt{\#define}. De esta forma, lo que previamente sería \texttt{layerN\_weights}, y un \textit{string} de datos binarios a continuación, se divide ahora en \texttt{layerN\_sp\_weights} (\textit{layerN sparse weights}, que contiene los valores distintos de cero para la matriz de pesos dispersa), así como en dos vectores de coordenadas \texttt{layerN\_i} y \texttt{layerN\_j} (fila y columna de cada valor en el array de pesos, respectivamente).
    
    Además, también es necesario el número de \textit{nonzeros} (\texttt{layerN\_nz}) para la creación de dichas matrices. Sabiendo esto, las matrices dispersas se crean de la siguiente manera:\medskip
\begin{lstlisting}[language=C]
blas_sparse_matrix layer1_sp_weights = blas_invalid_handle;
/* [...] */

layer1_sp_weights = BLAS_suscr_begin(LAYER0_SIZE, LAYER1_SIZE);
BLAS_suscr_insert_entries(layer1_sp_weights, layer1_nz, (float *)
                          layer1_weights, layer1_i, layer1_j);
BLAS_suscr_end(layer1_sp_weights);
\end{lstlisting}

    \item Finalmente, queda emplear las funciones adecuadas para la multiplicación de matrices $d\times D$. Este proceso consiste básicamente en sustituir la función \texttt{[sdcz]gemm} por una función \texttt{[sdcz]usmm} (ver Subsección \ref{ssec:propiedades_producto_matrices_dispersas}), así como indicar a la función \texttt{map\_and\_bias} que la matriz de resultados se encuentra transpuesta. Las dimensiones $M$ y $N$ de la matriz original no transpuesta \textit{C} también deben intercambiarse:\medskip
\begin{lstlisting}[language=C]
fprintf(stderr, "*** SUSMM %s(T) x %s(T) ***\n", "layer3", "layer2");
BLAS_susmm(blas_rowmajor, blas_trans, input_dim, 1.f, layer3_sp_weights,
           (float *) layer2_out, input_dim, (float *) layer3_out,
           input_dim);
fprintf(stderr, "*** Map and Bias %s(T) with %s function ***\n",
                "layer3_out", "sigmoid__fp32");
map_and_bias__fp32(layer3_out, layer3_bias, LAYER3_SIZE, input_dim,
                   'T', sigmoid__fp32);
\end{lstlisting}
\end{enumerate}

\subsection{Arquitectura \textit{point-to-point}}
\label{ssec:gdin_arquitectura_point_to_point}
Ahora que se ha visto cómo implementar una arquitectura basada en matrices densas, así como su adaptación a matrices dispersas, se procede a explicar cómo se ha implementado la arquitectura \textit{point-to-point}. Para ello, se ha empleado la misma estructura de código que la arquitectura basada en matrices densas, pero empleando secuencias \texttt{\acrshort{fma}} y lógica de punteros, tal como se comenta en la Sección \ref{sec:multiplicacion_point_to_point}. Para realizar esto, los pasos a seguir basándose en la implementación con matrices densas son los siguientes:

\begin{enumerate}
    \item Implementar la generación de código \textit{point-to-point}.
    \item Paralelizar el código generado con directivas \texttt{OpenMP}\footnote{\url{https://www.openmp.org}}.
    \item Aprovechando la inclusión de \texttt{OpenMP}, se paraleliza también la función \texttt{map\_and\_bias}.
\end{enumerate}

Debido a que tanto las operaciones \texttt{\acrshort{fma}} como la función \texttt{map\_and\_bias} son \textit{in-house}, no existe ninguna dependencia más allá de la librería matemática y las estándar, así como del soporte de \texttt{OpenMP} por parte del compilador. Para proceder con estas modificaciones se realizan los siguientes cambios:
\begin{enumerate}
    \item La implementación de la generación de código se realiza mediante un nuevo \textit{backend} en el \textit{Notebook} (fichero \texttt{.ipynb}), y debido a mejoras en la programación del mismo con respecto a versiones anteriores, se obtiene en un tiempo razonable un código similar al siguiente:\medskip
\begin{lstlisting}[language=C]
#pragma omp for
for(int k = 0; k < input_dim; k++){
    temp_input = layer0_out + k*LAYER0_SIZE;
    temp_output = layer1_out + k*LAYER1_SIZE;

    temp_output[0].val += layer1_weights[0].val * temp_input[13].val;
    temp_output[2].val += layer1_weights[1].val * temp_input[8].val;
    temp_output[2].val += layer1_weights[2].val * temp_input[22].val;
    temp_output[3].val += layer1_weights[3].val * temp_input[13].val;
    temp_output[4].val += layer1_weights[4].val * temp_input[3].val;
    temp_output[4].val += layer1_weights[5].val * temp_input[5].val;
}
\end{lstlisting}
    \item En el código anterior se realiza la multiplicación y suma de los valores estrictamente necesarios para cada capa. Al ser iteraciones independientes sobre cada fila de las matrices de entrada y salida, es muy fácilmente paralelizable con \texttt{OpenMP}. El código auxiliar, como los mensajes de \textit{log} y cálculos de tamaños y \textit{strides} por proceso, también se deben crear o modificar. La implementación de \texttt{OpenMP} se ha realizado además de forma modular, permitiendo que si el código se compila sin soporte de esta librería, el funcionamiento no se vea afectado en absoluto.
    \item El punto anterior implica también una paralelización de la función \texttt{map\_and\_bias}. Los cambios a la estructura del código, incluyendo los cálculos auxiliares mencionados en el punto anterior, se resumen a continuación:\medskip
\begin{lstlisting}[language=C]
// En la cabecera se añade
#if defined(_OPENMP)
#include <omp.h>
#endif

// Se abre la zona paralela...
#pragma omp parallel private(temp_input, temp_output)
{
// y se realizan cálculos auxiliares para calcular la carga de cada hilo
#if defined(_OPENMP)
int num_threads = omp_get_num_threads();
int thread_id = omp_get_thread_num();

// Se redondea hacia arriba en los n primeros hilos,
//  tal que n = input_dim % num_threads
int start = (input_dim / num_threads) * thread_id + ((thread_id < input_dim % num_threads) ? thread_id : input_dim % num_threads);
int end = (input_dim / num_threads) * (thread_id + 1) + ((thread_id < input_dim % num_threads) ? thread_id + 1 : input_dim % num_threads);

fprintf(stderr, "*** [OpenMP] Thread %d: start = %d, end = %d ***\n", thread_id, start, end);
#else
int num_threads = 1;
int thread_id = 0;
int start = 0;
int end = input_dim;
#endif

#ifdef DEBUG_VERBOSE
// La información de depuración debe imprimirse solo una vez
#pragma omp master
fprintf(stderr, "*** POINT-TO-POINT MATMUL %s x %s ***\n", "layer0", "layer1");
#endif
// Y se paraleliza el for como se comenta en el punto 1
#pragma omp for
for(int k = 0; k < input_dim; k++){
    temp_input = layer0_out + k*LAYER0_SIZE;
    temp_output = layer1_out + k*LAYER1_SIZE;

    temp_output[0].val += layer1_weights[0].val * temp_input[0].val;
    //                            [...]
    temp_output[4].val += layer1_weights[119].val * temp_input[23].val;
}
#ifdef DEBUG_VERBOSE
#pragma omp master
fprintf(stderr, "*** Map and Bias %s(T) with %s function ***\n", "layer1_out", "sigmoid__fp32");
#endif
// Finalmente se paraleliza map_and_bias
map_and_bias__fp32(layer1_out+start*LAYER1_SIZE, layer1_bias, end-start, LAYER1_SIZE, 'N', sigmoid__fp32);
#pragma omp barrier

// Repetir el procedimiento N veces por N capas [...]
} // #pragma omp parallel
\end{lstlisting}
\end{enumerate}

\section{Posibilidades de la generación de código}
\label{sec:posibilidades_de_la_generacion_de_codigo}
Las posibilidades de esta aproximación al análisis y medida del rendimiento son muchas. Generar código C a partir de un modelo que precisa de soporte en Python es algo especialmente útil para sistemas sin dicho soporte. Es cierto que existe TensorFlow Lite\footnote{\url{https://www.tensorflow.org/lite}} para sistemas embebidos, pero la flexibilidad que aporta crear un código C y la posibilidad de paralelizar dicho código tanto en memoria compartida como distribuida son motivos de peso.

En conjunción con todo esto, el hecho de emplear matrices dispersas viene no solamente condicionado por la literatura ya existente, que destaca los beneficios en cuanto a la proporción rendimiento/precisión de redes podadas, sino que viene también motivado por las labores de investigación en cuanto a análisis estático de código y empaquetamiento de operandos para vectorización conducidas en \cite{custom_high_performance_vector_codegen_sparse_computations}.

A continuación, se mencionan algunas de las posibilidades que abre esta generación de código a partir de un modelo preentrenado.

\subsection{En plataformas de cómputo generalistas}
\label{ssec:posibilidades_en_computo_generalistas}
En el campo de la inteligencia artificial, tanto el entrenamiento como la inferencia de redes neuronales están dominados por las \acrshort{gpu}s (\textit{\acrlong{gpu}s}). Estos dispositivos son muy convenientes para las cargas de trabajo que se manejan en \textit{deep learning}, y aportan aceleraciones muy significativas sobre su contraparte en CPU. Sin embargo, los principales fabricantes de CPUs están realizando fuertes inversiones en la adaptación de sus arquitecturas tradicionales para la aceleración de \textit{workloads} de \acrshort{ia}. Fabricantes como Intel y AMD, pero también algunos otros como ARM o Qualcomm investigan para mejorar el rendimiento y, sobre todo en el caso de estos últimos, la eficiencia energética de sus procesadores.

Dada esta situación, y a pesar de que librerías como \texttt{numpy} están altamente optimizadas, es cierto que incorporando la investigación acerca del empaquetamiento de operandos para operaciones con matrices dispersas realizada en \cite{custom_high_performance_vector_codegen_sparse_computations}, es posible que se mejore el rendimiento y consumo energético de una red basada en matrices dispersas en una plataforma de cómputo generalista basada en CPU con extensiones vectoriales (tales como SSE y AVX\footnote{\url{https://en.wikipedia.org/wiki/Advanced\_Vector\_Extensions}} de Intel, o Neon\footnote{\url{https://developer.arm.com/Architectures/Neon}} de ARM).

Por esta razón, a pesar de que a día de hoy es difícil mejorar el rendimiento de una red dispersa sin afectar a su precisión, incorporando estas nuevas técnicas se podría obtener o bien una mayor precisión con igual rendimiento, o bien un mayor rendimiento con igual precisión. Además, esta inferencia podría realizarse en entornos de memoria compartida o distribuida, únicamente alterando el correspondiente \textit{backend}.


\subsection{En plataformas de cómputo heterogéneas}
\label{ssec:posibilidades_en_computo_heterogeneas}
Siguiendo la línea de la subsección anterior, estas implementaciones tanto en memoria compartida como distribuida podrían hacer uso de diversos aceleradores. Ante esta posibilidad, aparecen múltiples tipos de aceleradores válidos para este cometido.

\subsubsection{GPU}
\label{sssec:heterogeneas_gpu}
Como ya se comentó previamente en la Subsección \ref{ssec:posibilidades_en_computo_generalistas}, las GPUs son piezas de hardware excelentes para la inferencia, por lo que contar con una mayor cantidad de ellas en un entorno escalable de memoria distribuida es algo deseable.

Generar código para estos dispositivos, si bien cuenta con sus peculiaridades en la implementación y optimización, sería posible empleando un \textit{backend} para CUDA, ROCm, OpenCL y similares.

\subsubsection{FPGA}
\label{sssec:heterogeneas_fpga}
Las \acrshort{fpga}s o \textit{\acrlong{fpga}s} son dispositivos programables a nivel de hardware. Si bien es conveniente no entrar en detalles de implementación, la posibilidad de cambiar las rutas por donde fluyen los datos, así como de interconectar diferentes piezas de hardware como memorias, \acrshort{cpu}s y más recursos hardware según sea más conveniente, permite un aprovechamiento sin igual de su arquitectura, mediante por ejemplo la creación de \textit{pipelines}.

Si bien conseguir una buena arquitectura no es trivial, sabiendo que es posible reducir las operaciones con matrices dispersas a una secuencia finita de operaciones \textit{hardcodeadas}, no resulta descabellado pensar en la opción de implementar dicho \textit{backend} para \acrshort{fpga}s mediante una arquitectura adecuada.

\subsubsection{ASIC}
\label{sssec:heterogeneas_asic}
El siguiente paso en el aumento de rendimiento (absoluto o por unidad de energía) es recurrir a circuitos de aplicación específica, \acrshort{asic}s (\textit{\acrlong{asic}s}). Con el término \acrshort{asic} se engloba no solo a los chips diseñados de cero con una finalidad, sino también a los procesadores de aplicación específica o \acrshort{asip}s (\textit{\acrlong{asip}s}). Estos circuitos programados en una \acrshort{isa} (\textit{\acrlong{isa}}) específica y adaptada a las necesidades de la carga de trabajo, o directamente diseñados en un lenguaje de diseño hardware como VHDL o Verilog, permiten ese extra de eficiencia necesario en sistemas embebidos o de muy alta replicación y, por tanto, de muy alto consumo agregado.

Como se puede imaginar, si bien difícil, no es imposible implementar un \textit{backend} que exporte modelos TensorFlow a VHDL o Verilog para la generación de \acrshort{asic}s. Tampoco debería ser particularmente difícil compilar código C para \acrshort{asip}s, puesto que el código de multiplicación de matrices dispersas se puede reducir a una secuencia finita de operaciones \texttt{\acrshort{fma}} (tal como se explica en la Sección \ref{sec:multiplicacion_point_to_point} y se detalla su implementación en la Subsección \ref{ssec:gdin_arquitectura_point_to_point}). Las optimizaciones, sin embargo, deberían venir de cómo se implementa el producto de matrices en hardware, así como otras operaciones relevantes, como la función de transferencia sigmoide u otras alternativas ampliamente utilizadas.

Por último, en estos sistemas embebidos podría ser posible emplear otras técnicas de bajo nivel, imposibles de ejecutar en un ordenador convencional, tales como permitir cierta tasa de error en los accesos a memoria a cambio de una reducción en el voltaje operativo de la misma (ver Subsección \ref{ssec:otras_optimizaciones}).
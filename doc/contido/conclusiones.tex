\chapter{Conclusiones}
\label{chap:conclusiones}

\lettrine{E}{n} este último capítulo se realiza un balance general de este \acrlong{tfm}, se comenta brevemente la relación con la titulación, y se tratan las líneas de investigación futuras.

\section{Conclusiones}
El objetivo principal de este trabajo era el análisis, así como una posible mejora de los tiempos de ejecución de modelos de inteligencia artificial en fase de inferencia, así como de su consumo energético. Estos objetivos se han cumplido satisfactoriamente. No solamente se ha analizado en detalle el producto de matrices para inteligencia artificial, siendo este código una porción muy relevante del tiempo de ejecución de múltiples arquitecturas de redes neuronales, sino que se ha implementado de cero y en código C una red neuronal, sobre la que se han propuesto mejoras útiles bajo ciertas condiciones de \textit{sparsity}.

Los resultados obtenidos son buenos y esperanzadores, ya que mediante una paralelización sencilla, ausencia de vectorización y una ordenación de datos no necesariamente optimizada, se obtienen, a partir de aproximadamente el 87,5\% de \textit{sparsity}, tiempos de ejecución crecientemente mejores con respecto a los obtenidos por librerías \acrshort{blas} estándar en la industria. Este nivel de \textit{sparsity} es algo elevado para una red de propósito general, por lo que las optimizaciones propuestas, de momento, no pueden ser empleadas en la resolución de cualquier problema con un índice de dispersión más modesto. Sin embargo, tal como se puede apreciar en la Figura \ref{fig:grafica_sparse_vs_dense} (Subsección \ref{ssec:podado_y_redes_dispersas}), para una red correctamente diseñada y entrenada, es en los valores intermedios entre 80\% y 90\% de \textit{sparsity} donde se obtiene la mejor relación entre rendimiento y precisión.

Por último, y siendo este un objetivo secundario pero no por ello menos importantes, y si bien la medida del consumo energético no se ha realizado detalladamente, resulta evidente que, tal como se comenta en capítulos anteriores, como por ejemplo en la Subsección \ref{ssec:xpu}, una menor cantidad de transferencias desde memoria principal, así como una menor cantidad de operaciones en CPU, siempre que el diseño de la microarquitectura acompañe, debe necesariamente traducirse en una menor cantidad de energía consumida.

\section{Relación con la titulación}
En este trabajo se han ampliado extensivamente herramientas de depuración y perfilado, tratadas principalmente en la asignatura de Herramientas para HPC, así como una (simple) paralelización con \texttt{OpenMP} del código \textit{point-to-point}. Esta disciplina, si bien no se ha necesitado un nivel de maestría elevado con el framework en cuestión, ha sido adquirida en las asignaturas de Programación Paralela y Programación Paralela Avanzada.

Por otro lado, el hecho de poder plantear ciertos razonamientos con respecto a la correcta utilización de la jerarquía de memoria, así como la implementación de un algoritmo optimizado para ello, no sería posible sin el trabajo realizado durante los últimos años tanto en el Grado en Ingeniería Informática como en este Máster en Computación de Altas Prestaciones.

\section{Trabajo futuro}
Las líneas de trabajo futuro se mencionan varias veces a lo largo del documento. Estando además este trabajo orientado a futura investigación, continuar la optimización del código \textit{point-to-point} ha estado siempre en el horizonte cercano. Mediante el empleo de técnicas de \textit{Data Mining}, el uso de la herramienta MACVETH, así como mejorando la ubicación de los operandos en memoria para un mejor uso del ancho de banda de la memoria principal y la localidad caché, se espera una sustancial mejora en los rendimientos para índices de dispersión donde ya se supera a \texttt{OpenBLAS}, así como continuar expandiendo la viabilidad de esta aproximación para densidades mayores. En paralelo a estas labores más profundas de investigación y mejora de la herramienta, mejorarla es una prioridad, haciendo así que deje de ser una simple Prueba de Concepto. Esto se realizaría en varios pasos: 
\begin{itemize}
    \item Conversión del \textit{Jupyter Notebook} a una herramienta universal, programada en Python de inicio a fin, que lea una red neuronal como entrada y expulse código como salida. Esta herramienta podría ser tanto interactiva como no interactiva.
    \item Mejora en la modularización de la herramienta y generación de código. Ahora mismo los \textit{backends} implementados son engorrosos, lentos, y poco mantenibles, por lo que necesitarían una puesta a punto. Además, es conveniente dividir el código en varias secciones lógicas, para poder realizar la compilación y optimización en diversas \textit{translation units}\footnote{\url{https://en.wikipedia.org/wiki/Translation_unit_(programming)}}, y así aprovechar los múltiples núcleos que ofrece un procesador moderno.
    \item Implementar un mecanismo automatizado sobre esta herramienta mejorada, en la que se pueda seguir un \textit{workflow} ágil, pudiéndole suministrar enlaces a redes en formato \textit{ONNX} para la conversión a uno o más ficheros C, que sean analizados, optimizados, y compilados.
    \item Mejoría en la obtención de métricas, mejorando el actual sistema de medición de tiempos integrando, por ejemplo, un sistema de medición de contadores PAPI.
    \item Implementación de otros \textit{backends} para arquitecturas ya existentes como para arquitecturas \textit{ad hoc}, por ejemplo, en ensamblador o VHDL.
    \item Implementación de soporte para memoria distribuida mediante MPI\footnote{\url{https://www.mpi-forum.org/docs/}} o similar.
\end{itemize}

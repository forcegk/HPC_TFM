\chapter{Conclusiones}
\label{chap:conclusiones}

\lettrine{E}{n} este último capítulo se hace un balance general de este \acrlong{tfm}, se comenta brevemente la relación con la titulación, y posibles líneas de investigación futuras.

\section{Conclusiones}
El objetivo principal de este trabajo era el análisis, así como una posible mejora de los tiempos de ejecución de modelos de inteligencia artificial en fase de inferencia o de su consumo energético. Estos objetivos se han cumplido satisfactoriamente, ya que no solamente se ha analizado en detalle el producto de matrices para inteligencia artificial, siendo este código una porción muy relevante del tiempo de ejecución de múltiples arquitecturas de redes neuronales, sino que se ha implementado de cero y en código C una red neuronal, sobre la que se han propuesto mejoras útiles bajo ciertas condiciones de \textit{sparsity}. La medida del consumo energético no se ha realizado detalladamente, pero resulta evidente que, tal como se comenta en capítulos anteriores, una menor cantidad de transferencias desde memoria principal debe necesariamente traducirse en una menor cantidad de energía consumida.

\section{Relación con la titulación}
En este trabajo se han ampliado extensivamente herramientas de depuración y perfilado, tratadas principalmente en la asignatura de Herramientas para HPC, así como una (simple) paralelización con OpenMP del código \textit{point-to-point}. Esta disciplina, si bien no se ha necesitado un nivel de maestría elevado con el framework en cuestión, ha sido adquirida en las asignaturas de Programación Paralela y Programación Paralela Avanzada.

Por otro lado, el hecho de poder plantear ciertos razonamientos con respecto a la correcta utilización de la jerarquía de memoria, así como la implementación de un algoritmo optimizado para ello, no sería posible sin el trabajo realizado durante los últimos años tanto en el Grado en Ingeniería Informática como en este Máster en Computación de Altas Prestaciones.

\section{Trabajo futuro}
La línea de trabajo futuro se menciona varias veces a lo largo del documento, es una, y es clara: continuar la optimización del código \textit{point-to-point} mediante el uso de MACVETH, mejorando la ubicación de los operandos en memoria para un mejor uso del ancho de banda de la memoria principal, así como de la localidad en caché. En paralelo a estas labores más profundas de mejora, sería conveniente universalizar la herramienta y mejorarla, dejando de ser una simple \acrshort{poc}. Esto se realizaría en varios pasos: 
\begin{itemize}
    \item Conversión del \textit{Jupyter Notebook} a una herramienta ``en condiciones'', programada en Python de inicio a fin, que lea una red neuronal como entrada y expulse código como salida. Esta herramienta podría ser tanto interactiva como no interactiva.
    \item Mejora en la modularización de la herramienta y generación de código. Ahora mismo los \textit{\gls{backend}s} implementados son engorrosos, lentos, y poco mantenibles, por lo que necesitarían una puesta a punto. Además, es conveniente dividir el código en varias secciones lógicas, para así poder realizar la compilación y optimización en diversas \textit{translation units}\footnote{\url{https://en.wikipedia.org/wiki/Translation_unit_(programming)}}, y aprovechar los múltiples núcleos que ofrece un procesador moderno.
    \item Implementar un mecanismo automatizado sobre esta herramienta mejorada, en la que se pueda seguir un \textit{workflow} ágil, pudiéndole suministrar enlaces a redes en formato \textit{ONNX} para la conversión a uno o más ficheros C, que sean analizados, optimizados, y compilados.
    \item Mejoría en la obtención de métricas, mejorando el actual sistema de medición de tiempos integrando, por ejemplo, un sistema de medición de contadores PAPI.
    \item Implementación de otros \textit{\gls{backend}s} para arquitecturas ya existentes como para arquitecturas \textit{ad hoc}, por ejemplo, en ensamblador o VHDL.
    \item Implementación de soporte para memoria distribuida mediante MPI\footnote{\url{https://www.mpi-forum.org/docs/}} o similar.
\end{itemize}

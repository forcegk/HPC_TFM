\chapter{Conclusiones}
\label{chap:conclusiones}

\lettrine{E}{n} este último capítulo se extraen las principales conclusiones de este \acrlong{tfm}, se comenta brevemente su relación con la titulación, y se tratan las líneas de investigación futuras.

\section{Conclusiones}
El objetivo principal de este trabajo ha consistido en el análisis, así como en una posible mejora de los tiempos de ejecución de modelos de inteligencia artificial en fase de inferencia y su consumo energético. Estos objetivos se han cumplido satisfactoriamente. No solamente se ha analizado en detalle el producto de matrices en este contexto, siendo este código una porción muy relevante del tiempo de ejecución de múltiples arquitecturas de redes neuronales, sino que se ha implementado de cero y en código C una red neuronal, sobre la que se han propuesto mejoras útiles bajo ciertas condiciones de \textit{sparsity}.

Los resultados obtenidos son buenos y esperanzadores, ya que mediante una paralelización sencilla, ausencia de vectorización y una ordenación de datos no necesariamente optimizada, se obtienen, a partir de aproximadamente el 87,5\% de \textit{sparsity}, tiempos de ejecución crecientemente mejores con respecto a los obtenidos por librerías \acrshort{blas} estándar en la industria. Este nivel de \textit{sparsity} es algo elevado para una red de propósito general, por lo que las optimizaciones propuestas, de momento, no pueden emplearse en la resolución de cualquier problema con un índice de dispersión más modesto. Sin embargo, tal como se puede apreciar en la Figura \ref{fig:grafica_sparse_vs_dense} (Subsección \ref{ssec:podado_y_redes_dispersas}), para una red correctamente diseñada y entrenada, es en los valores intermedios entre el 80\% y 90\% de \textit{sparsity} donde se obtiene la mejor relación entre rendimiento y precisión.

Por último, una conclusión que se puede derivar de estos resultados es en relación a la posible mejora del consumo energético de redes neuronales dispersas. Y es que resulta evidente que, como por ejemplo se comentó en la Subsección \ref{ssec:xpu}, una menor cantidad de transferencias desde memoria principal, así como una menor cantidad de operaciones en CPU, debe necesariamente traducirse en una menor cantidad de energía consumida, siempre que el diseño de la microarquitectura acompañe.

\section{Relación con la titulación}
En este trabajo se han empleado extensivamente herramientas de depuración y perfilado, tratadas principalmente en la asignatura de Herramientas para HPC. También se ha realizado una paralelización con \texttt{OpenMP} del código \textit{point-to-point}, conocimiento adquirido en las asignaturas de Programación Paralela y Programación Paralela Avanzada.

Por otro lado, el hecho de poder plantear ciertos razonamientos con respecto a la correcta utilización de la jerarquía de memoria, así como la implementación de un algoritmo optimizado para ello, no sería posible sin el trabajo realizado durante los últimos años tanto en la Especialidad de Ingeniería de Computadores del Grado en Ingeniería Informática como en este Máster en Computación de Altas Prestaciones y, más concretamente, en la asignatura de Arquitecturas de Altas Prestaciones.

\section{Trabajo futuro}
Las líneas de trabajo futuro se mencionan varias veces a lo largo de la memoria. Dado que este trabajo está orientado a futura investigación en el marco de una tesis doctoral, continuar con la optimización del código \textit{point-to-point} ha estado siempre en el horizonte cercano. Mediante el empleo de técnicas de \textit{data mining}, el uso de la herramienta MACVETH, así como mejorando la ubicación de los operandos en memoria para un mejor uso del ancho de banda de la memoria principal y la localidad caché, se espera una mejora sustancial en los rendimientos para índices de dispersión donde ya se supera a \texttt{OpenBLAS}, así como continuar expandiendo la viabilidad de esta aproximación para densidades mayores. En paralelo a estas labores de investigación y optimización de la herramienta, mejorarla para su uso por parte de otros investigadores es una prioridad, haciendo así que deje de ser una simple Prueba de Concepto. Esto se realizaría en varios pasos: 
\begin{itemize}
    \item Conversión del \textit{Jupyter Notebook} a una herramienta universal, programada en Python de inicio a fin, que lea una red neuronal como entrada y genere código como salida. Esta herramienta podría ser tanto interactiva como no interactiva.
    \item Mejora en la modularización de la herramienta y generación de código. Ahora mismo los \textit{backends} implementados son lentos, y poco mantenibles, por lo que necesitarían una puesta a punto. Además, es conveniente dividir el código en varias secciones lógicas, para poder realizar la compilación y optimización en diversas \textit{translation units}\footnote{\url{https://en.wikipedia.org/wiki/Translation_unit_(programming)}}, y así aprovechar los múltiples núcleos que ofrece un procesador moderno.
    \item Implementar un mecanismo automatizado sobre la herramienta, de tal forma que se pueda seguir un \textit{workflow} ágil, pudiéndole suministrar enlaces a redes en formato \textit{ONNX} para la conversión a uno o más ficheros C, que sean analizados, optimizados y compilados.
    \item Mejora en la obtención de métricas, particularmente en el actual sistema de medición de tiempos integrando, por ejemplo, un sistema de medición de contadores PAPI.
    \item Implementación de otros \textit{backends} tanto para arquitecturas ya existentes como para arquitecturas \textit{ad hoc}, por ejemplo, en ensamblador o VHDL.
    \item Implementación de soporte para memoria distribuida mediante MPI\footnote{\url{https://www.mpi-forum.org/docs/}}.
\end{itemize}

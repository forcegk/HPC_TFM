\chapter{Conceptos básicos}
\label{chap:conceptos_basicos}

\lettrine{P}{ara} una correcta comprensión de los objetivos, tanto a corto como a largo plazo, de este trabajo, es necesario entender los conceptos básicos sobre los que se apoya esta línea de investigación.

\section{Redes neuronales}
\label{sec:redes_neuronales}
Las redes neuronales constituyen la base de la mayoría de últimos avances en el campo de la inteligencia artificial, y a pesar de la enorme diversidad en \textit{layouts} de capas, neuronas, funciones de transferencia, etc, la realidad es que el álgebra lineal y la multiplicación de matrices son parte fundamental e imprescindible para la ejecución de las mismas. \cite[Figure 3.4]{deep_learning_for_computer_architects}

A pesar de que en la \acrshort{poc} (\acrlong{poc}) expuesta más adelante en este trabajo se trabaja únicamente con redes \textit{Feed-Forward}, sería naíf pensar que solamente existen estas arquitecturas de redes neuronales. Ejemplos a destacar serían las redes convolucionales, empleadas principalmente para el procesado de imágenes, las basadas en modelos secuenciales o en transformers, que están revolucionando el mundo de la inteligencia artificial mediante modelos de procesamiento del lenguaje o de generación de imágenes, así como muchas otras como las redes GAN, o las basadas en Encoder-Decoder.

Estas arquitecturas son llamadas de redes neuronales, a pesar de que como es evidente, una neurona no puede, como tal ``ejecutar'' una convolución. Más bien, estas nuevas arquitecturas se basan en la modelación de operaciones matemáticas complejas en pasos discretos como, por ejemplo, la convolución que se menciona previamente.

\subsection{Estructura de una red neuronal feed-forward}
\label{ssec:estructura_red_neuronal_ff}
Las redes neuronales \textit{feed-forward} se componen de varias capas de neuronas que toman una o más entradas, realizan la suma de todas ellas, suman un \textit{bias}, y finalmente aplican una función de transferencia no lineal.

La capa que toma los datos de entrada se llama capa \textit{input} o de entrada, y la capa que expulsa los datos de salida es la capa \textit{output} o de salida. Las capas que realizan transformaciones intermedias se denominan capas ocultas, o \textit{hidden layers}.

\textbf{TODO insertar imagen con explicación}

\subsection{Redes neuronales profundas}
\label{ssec:redes_reuronales_profundas}
Dentro de las infinitas posibilidades que nos ofrecen las ``neuronas'' tratadas anteriormente, existe la posibilidad de apilar capa sobre capa, obteniendo a cada capa extra comportamientos y patrones más complejos.

Y es que, debido a la no-linealidad de las funciones de transferencia, no es posible reducir múltiples capas de comportamiento no lineal en una sola capa. En precisamente este comportamiento de los sistemas no lineales el que permite reproducir comportamientos exóticos al aumentar el número de capas (y por tanto la complejidad) del sistema.
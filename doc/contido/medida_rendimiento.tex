\chapter{Medida del rendimiento y perfilado}
\label{chap:medida_rendimiento_perfilado}

\lettrine{E}{n} este capítulo, tras plantear la problemática y documentar cómo se construye la prueba de concepto en los capítulos anteriores, se muestran los resultados de las medidas de rendimiento tanto para los códigos densos como dispersos, tanto basados en librería como en arquitectura punto a punto. Estos resultados se comparan con los que se pueden ver en la literatura ya existente, para comprobar si la red implementada se adhiere a los patrones indicados, por ejemplo, en la Sección \ref{sec:investigacion_optimizaciones_propuestas}.

\section{Metodología}
\label{sec:metodologia}
La medida del rendimiento de las pruebas de concepto se realiza por partes mediante el empleo del reloj monotónico del sistema en microsegundos. Al ser sobre todo los códigos densos y de librería simples pruebas de concepto, funciones auxiliares como por ejemplo \texttt{map\_and\_bias} no está debidamente paralelizada con OpenMP, a diferencia de la versión punto a punto, donde realizar esto es tarea trivial partiendo de la base de que OpenMP ya es necesario para el funcionamiento base. Realizar esta optimización no es difícil, es algo innecesario cuando lo que se pretende es medir las posibilidades de una aproximación, centrándose en el producto de matrices, y no optimizar y trabajar a fondo con una implementación en particular.

Por esta razón se ha decidido simplemente implementar de forma sencilla la generación de código con las librerías OpenBLAS y librsb sin añadir OpenMP, y la punto a punto completamente paralelizada con OpenMP. El \textit{overhead} que añade el procesado auxiliar de datos de forma secuencial es creciente conforme se aumentan el número de procesadores paralelos. Sin embargo, debido a que las mediciones de tiempos se realizan de forma aislada, estas no precisan de paralelización en un entorno de pruebas.

\subsection{Común}
\label{ssec:comun_metodologia}
Para la medida del rendimiento y perfilado se generan dos redes neuronales de un tamaño absurdamente grande, una densa con capas completamente conexas con un anchos tal que \{capa de entrada, $n\:\times\:$capas ocultas, capa de salida\} = \{24, 500, 800, 1000, 1200, 600, 400, 200, 100, 50, 1\}, y varias redes dispersas basadas en esta primera, con índices de dispersión del 60\%, 70\%, 80\%, 85\%, 90\%, 95\% y 99\%. Evidentemente, para el problema que se pretende resolver, estas redes están completamente sobredimensionadas y son cuanto menos inútiles, puesto que debido a su enorme tamaño lo único que aprenden durante el proceso de entrenamiento es a marcar como positivos todos los \textit{inputs}.

Esto, que para un ingeniero en inteligencia artificial sería un enorme fracaso, en este ámbito es algo completamente indiferente, ya que a pesar de la inutilidad de la red creada, ésta sigue realizando las cargas de trabajo típicas de una red neuronal adecuada. Esto es, la red tiene que multiplicar matrices, sumar los \textit{bias} y aplicar las funciones de transferencia.

En todas las ejecuciones se emplean los mismos datos de entrada replicados $n$, basados en el fichero \texttt{input.txt} generado con la función tratada en el Punto 3 de la Sección \ref{ssec:extraccion_valores}. En concreto, estos datos se replican 100 y 1000 veces, para obtener $1000 \times 100 = 100000$ y $1000 \times 1000 = 1000000$ datos de entrada.

\subsection{Medida del rendimiento}
\label{ssec:medida_rendimiento_metodologia}
Para la medida del rendimiento se emplea el reloj monotónico del sistema mediante la función \texttt{clock\_gettime(CLOCK\_MONOTONIC\_RAW, \&timestamp)} en compilaciones de \textit{release} (opción \texttt{-s} o \textit{strip}). Se realizan cinco ejecuciones para cada red y \textit{sparsity}, y se guarda la salida de \texttt{stdout} con el script \texttt{run\_tests.sh}:\medskip
\begin{lstlisting}[language=bash]
#!/bin/sh
mkdir -p logs
INPUT=inputs/input_long.txt

for it in 1 2 3 4 5; do
./dense.out $INPUT > logs/dense.$it.log
for sp in 60 70 80 85 90 95 99; do
    echo RUNNING LIBRSB FOR $sp% SPARSITY $it ITERATION
    ./sparse_$sp.out $INPUT > logs/sparse_$sp.$it.log

    echo RUNNING P2P FOR $sp% SPARSITY $it ITERATION
    ./p2p_$sp.out $INPUT > logs/p2p_$sp.$it.log
done
done  
\end{lstlisting}

Las ejecuciones para medición de tiempos se realizan en la partición \texttt{compute2} del clúster Plutón\footnote{\url{https://pluton.dec.udc.es/guide/user-guide.pdf}}, con características visibles en la Tabla \ref{tb:especificaciones_compute2}. Los perfilados, sin embargo, se realizan en un equipo Xiaomi Mi Notebook 15 con las características visibles en la Tabla \ref{tb:especificaciones_xiaomi}.
\begin{table}
    \centering
    \begin{tabular}{|c|c|}
        \hline
        CPU & 2x Intel Xeon Silver 4216 (2x16C/32T) @ 3.2GHz\\\hline
        RAM & 256GB DDR4 2933Mhz (8 $\times$ 32GB)\\\hline
        Sistema & CentOS Linux release 7.9.2009 (Core) \\\hline
        Kernel & 3.10.0-1160.76.1.el7.x86\_64 \\\hline
        CC & gcc 9.3.0\\\hline
        OpenBLAS & OpenBLAS 0.3.21\\\hline
        librsb & librsb 1.3.0.1\\\hline
    \end{tabular}
    \caption{\label{tb:especificaciones_compute2}Especificaciones técnicas de un equipo en la partición \texttt{compute2} del clúster Plutón}
    \end{table}

\begin{table}
\centering
\begin{tabular}{|c|c|}
    \hline
    CPU & 11th Gen Intel(R) Core(TM) i7-11370H @ 3.3GHz\\\hline
    RAM & 16GB DDR4 3200MHz (\textit{Row-of-chips} {\small$\approx$} 2 $\times$ 8GB)\\\hline
    Sistema & Ubuntu 20.04 LTS\\\hline
    Kernel & TODO COMPLETAR \\\hline
    CC & gcc 11 TODO CHECKEAR VERSIÓN\\\hline
    OpenBLAS & libopenblas-dev focal, v0.3.8\\\hline
    librsb & librsb-dev focal, v1.2.0.8\\\hline
    oneAPI & Intel oneAPI 2022\\\hline
\end{tabular}
\caption{\label{tb:especificaciones_xiaomi}Especificaciones técnicas del Xiaomi Mi Notebook 15}
\end{table}


\subsection{Análisis y perfilado}
\label{ssec:analisis_perfilado_metodologia}
Para el análisis y perfilado se emplea el programa Intel Advisor, el cual permite realizar modelos \textit{roofline} de partes individualizadas de programas, incluyendo las funciones de su librería \acrshort{mkl} (\textit{\acrlong{mkl}}). Esto es especialmente útil para perfilar tanto la implementación densa, que emplea funciones de \texttt{cblas} ampliamente utilizadas, así como la punto a punto, que no emplea ninguna llamada a librería.

Sin embargo, esta universalidad se pierde con la versión dispersa (\textit{sparse}), ya que se emplean funciones \textit{built-in} de OpenBLAS, así como funciones propias de librsb, lo que hace que cambiar a la \acrshort{mkl} requiera una reprogramación de ciertas líneas del código para poder perfilar las funciones de librería con Intel Advisor. Esto es algo costoso que inicialmente puede parecer un problema, pero es un trabajo que se puede evitar si se razona con respecto al modelo \textit{roofline} de la versión densa.

\subsection{Compilación}
\label{ssec:compilacion_metodologia}
Para compilar las versiones densa y dispersa se requiere intercambiar OpenBLAS por la Intel \acrshort{mkl}, así como desactivar el \textit{stripping} del binario para activar los símbolos de depuración (susituir parámetro \texttt{-s} por \texttt{-g}). La versión punto a punto no requiere modificaciones significativas ya que únicamente emplea librerías estándar. En cuanto a las funciones de \acrshort{blas}, la modificación de las líneas de compilación genéricas que se pueden encontrar en el fichero \texttt{.ipynb} se muestra estructurado a continuación.

\subsubsection{Código denso}
Para la obtención del \textit{roofline model} de la carga de trabajo, es necesario o bien calcularlo manualmente, o bien emplear alguna herramienta adecuada para ello. Como ya se comenta previamente, se emplea Intel Advisor para el perfilado del código, por lo que es necesario compilar el código denso con una configuración que sustituya OpenBLAS por \acrshort{mkl}. Para esto se puede compilar de las siguientes formas:\medskip
\begin{lstlisting}[language=bash]
# Para una compilación convencional sin depuración con OpenBLAS, sería necesario únicamente ejecutar
gcc -march=native -O3 -s common.c dense.c -o dense.out -lm -lopenblas

# Sin embargo, con propósitos de perfilado con Intel Advisor, en un entorno bash donde se haya realizado `source /opt/intel/oneapi/setvar.sh` se ha de compilar con:
gcc -march=native -O3 -g3 -DMKL_ILP64 -m64 -I"${MKLROOT}/include" common.c dense.c -o dense.out -L${MKLROOT}/lib/intel64 -Wl,--no-as-needed -lmkl_intel_ilp64 -lmkl_gnu_thread -lmkl_core -lgomp -lpthread -lm -ldl
\end{lstlisting}

\subsubsection{Código \textit{sparse}}
En este caso, debido al uso de \texttt{librsb} como librería de Sparse BLAS, la herramienta de perfilado y análisis de código Intel Adviso no es capaz de analizar el código de librerían ni compilando la misma con flags de \textit{debug}. Como se comenta previamente, una adaptación a la librería Intel MKL, a pesar de no ser imposible, no es conveniente. Por esto mismo más adelante en este capítulo se estima el rendimiento y posibilidades de mejora del código disperso, en función a los resultados con respecto al denso. Para compilar el código para \textit{release}, las líneas de compilación son las siguientes:\medskip
\begin{lstlisting}[language=bash]
# Para una compilación convencional sin depuración con OpenBLAS, sería necesario únicamente ejecutar
gcc -march=native -O3 -s common.c dense.c -o sparse.out -lm -lrsb -lopenblas
\end{lstlisting}

\subsubsection{Código \textit{point-to-point}}
En este caso solamente se emplean librerías estándar, por lo que en las líneas de compilación simplemente se cambia \texttt{-s} por \texttt{-g3}:\medskip
\begin{lstlisting}[language=bash]
# Para una compilación con OpenMP
gcc -march=native -O3 -g3 common.c p2p.c -o p2p.out -lm -fopenmp

# Para una compilación sin OpenMP
gcc -march=native -O3 -g3 common.c p2p.c -o p2p.out -lm -fno-openmp
\end{lstlisting}

\section{Medida de rendimiento}
\label{sec:medida_rendimiento}
En esta sección se muestra una comparación de rendimientos entre las diferentes implementaciones o \textit{\gls{backend}s} para cada \textit{sparsity} y aproximación usada (OpenBLAS, librsb o p2p), basados en medidas obtenidas según la metodología descrita en la sección anterior.

En primer lugar, en la Figura \ref{fig:rendimiento_100k} se muestran los resultados para 100k datos de entrada. En ésta se puede observar como la aproximación punto a punto supera ampliamente a OpenBLAS a partir del 87,5\%, mientras que librsb no es capaz de superar a OpenBLAS en ningún caso, lo que quizás indique que esta librería está enfocada a matrices con un índice de dispersión mayor.

En la Figura \ref{fig:rendimiento_1M} se pueden observar resultados muy similares, destacando quizás que acercándose a índices mayores de dispersión el código mejora mucho más que con una cantidad de datos inferior. Este comportamiento se puede apreciar más claramente en la Figura \ref{fig:cross_speedup_1M_100k}, donde se analiza el \textit{cross-speedup} entre la Figura \ref{fig:rendimiento_100k} y la Figura \ref{fig:rendimiento_1M}. Esto es, siendo $XS$ el \textit{cross-speedup}, $S(A)$ el speedup obtenido con $A$ elementos, se puede decir que la magnitud que se representa gráficamente se calcula tal que:
\begin{equation}
    XS = S(1M) / S(100k)\nonumber
    \label{eq:cross_speedup}
\end{equation}

Así, se puede apreciar que la mejoría del speedup con un dataset 10 veces más grande es cada vez más alta a partir del 90\% de \textit{sparsity}.

\begin{figure}[htpb]
\centering
\begin{tikzpicture}
    \begin{axis}[
        xmin = 55, xmax = 105, xlabel=\textit{sparsity},
        ymin = 0.01, ymax=19, ymode=log, log basis y=10, ylabel=\textit{speedup}, yticklabels={PADDING, {0,01}, {0,1}, 1, 10},
        width = \textwidth,
        height = 0.55\textwidth,
        grid=both,
        legend pos=north west,
    ]

    \addplot[udcgray,smooth,very thick,mark=*] coordinates {
        (60,1)
        (70,1)
        (80,1)
        (85,1)
        (90,1)
        (95,1)
        (99,1)
    };
    \addlegendentry{OpenBLAS}
    
    \addplot[ficblue,smooth,very thick,mark=*] coordinates {
        (60,0.042)
        (70,0.055)
        (80,0.074)
        (85,0.094)
        (90,0.127)
        (95,0.204)
        (99,0.511)
    };
    \addlegendentry{librsb}

    \addplot[udcpink,smooth,very thick,mark=*] coordinates {
        (60,0.262)
        (70,0.361)
        (80,0.574)
        (85,0.806)
        (90,1.253)
        (95,2.852)
        (99,9.985)
    };
    \addlegendentry{p2p}

    % Kinda ugly
    % \vasymptote[udcpink]{87.72}
    \end{axis}
\end{tikzpicture}
\caption{Speedups según \textit{sparsity} para 100k datos de entrada}
\label{fig:rendimiento_100k}
\end{figure}

\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xmin = 55, xmax = 105, xlabel=\textit{sparsity},
            ymin = 0.01, ymax=19, ymode=log, log basis y=10, ylabel=\textit{speedup}, yticklabels={PADDING, {0,01}, {0,1}, 1, 10},
            width = \textwidth,
            height = 0.55\textwidth,
            grid=both,
            legend pos=north west,
        ]
    
        \addplot[udcgray,smooth,very thick,mark=*] coordinates {
            (60,1)
            (70,1)
            (80,1)
            (85,1)
            (90,1)
            (95,1)
            (99,1)
        };
        \addlegendentry{OpenBLAS}
        
        \addplot[ficblue,smooth,very thick,mark=*] coordinates {
            (60,0.037)
            (70,0.050)
            (80,0.069)
            (85,0.087)
            (90,0.120)
            (95,0.201)
            (99,0.550)
        };
        \addlegendentry{librsb}
    
        \addplot[udcpink,smooth,very thick,mark=*] coordinates {
            (60,0.289)
            (70,0.399)
            (80,0.629)
            (85,0.850)
            (90,1.367)
            (95,3.352)
            (99,15.694)
        };
        \addlegendentry{p2p}
        \end{axis}
    \end{tikzpicture}
    \caption{Speedups según \textit{sparsity} para 1M de datos de entrada}
    \label{fig:rendimiento_1M}
\end{figure}

\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xmin = 55, xmax = 105, xlabel=\textit{sparsity},
            ymin = 0.8, ymax=1.7, ylabel=\textit{cross-speedup},
            width = \textwidth,
            height = 0.55\textwidth,
            grid=both,
            legend pos=north west,
        ]
    
        \addplot[udcgray,smooth,very thick,mark=*] coordinates {
            (60,1)
            (70,1)
            (80,1)
            (85,1)
            (90,1)
            (95,1)
            (99,1)
        };
        \addlegendentry{OpenBLAS}
        
        \addplot[ficblue,smooth,very thick,mark=*] coordinates {
            (60,0.879)
            (70,0.911)
            (80,0.928)
            (85,0.925)
            (90,0.942)
            (95,0.985)
            (99,1.076)
        };
        \addlegendentry{librsb}
    
        \addplot[udcpink,smooth,very thick,mark=*] coordinates {
            (60,1.105)
            (70,1.104)
            (80,1.096)
            (85,1.054)
            (90,1.091)
            (95,1.175)
            (99,1.572)
        };
        \addlegendentry{p2p}
        \end{axis}
    \end{tikzpicture}
    \caption{Cross-speedup según \textit{sparsity} de 1M con respecto a 100k datos de entrada}
    \label{fig:cross_speedup_1M_100k}
\end{figure}

\section{Perfilado y \textit{roofline model}}
\label{sec:perfilado_roofline}
En esta sección se muestran las características de cada carga de trabajo con ayuda de Intel Advisor, y se razonan los posibles resultados que no se han podido obtener debido a limitaciones en el análisis.


%%%%%%%%%%%%% COMPLETAR DESDE AQUÍ %%%%%%%%%%%%%

\subsection{Código denso}
Los resultados obtenidos para la red neuronal densa son los siguientes:

\begin{center}
Tiempo $(\overline{t} \pm \sigma)$ = 10,300 s $\pm$ 0,152 s

Rangos (min \ldots\ max) = 10,022 s \ldots\ 10,568 s
\end{center}

Estos resultados se obtienen además realizando un excelente uso de la memoria caché, tal como se muestra en el modelo \textit{roofline} en la Figura \ref{fig:roofline_dense}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{img/roofline_dense.png}
    \caption{\textit{Roofline model} del código denso}
    \label{fig:roofline_dense}
\end{figure}

Como se puede apreciar en el modelo, el \textit{workload} de interés, que es el que se puede encontrar en la parte superior derecha (coloreado en rojo y verde), corresponde a las funciones \texttt{cblas\_sgemm}. Estas funciones están fuertemente optimizadas y emplean instrucciones AVX512, como se puede observar en los detalles de la carga (Figura \ref{fig:roofline_dense_details}).

Fijándose con atención se pueden ver funciones con un considerablemente menor desempeño en la parte inferior izquierda. Estos puntos corresponden a \texttt{map\_and\_bias} y sucesivas llamadas a otras funciones como \texttt{expf}. Tal como se comenta previamente, una paralelización es sencilla de funciones auxiliares es sencilla. Sin embargo y tal como se puede ver en el modelo, se pueden distinguir perfectamente los componentes de dichas funciones, y siendo el tiempo de ejecución constante para dos redes con las mismas dimensiones, es sencillo discernir qué mejorías vienen causadas por un producto de matrices más eficiente.    

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{img/roofline_dense_details.png}
    \caption{Detalles de \texttt{cblas\_sgemm} en el modelo del código denso}
    \label{fig:roofline_dense_details}
\end{figure}

\subsection{Código \textit{sparse}}
Por otro lado, los resultados obtenidos para la red neuronal dispersa son los siguientes:

\begin{center}
Tiempo $(\overline{t} \pm \sigma)$ = 13,862 s $\pm$ 0,710 s

Rangos (min \ldots\ max) = 12,635 s \ldots\ 15,285 s
\end{center}

En este caso, debido al empleo de la librería librsb, el modelo \textit{roofline} no contiene información de utilidad (Figura \ref{fig:roofline_sparse_details}). Esto, que a todas luces es un problema, deja de serlo si se realiza un sencillo razonamiento.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{img/roofline_sparse.png}
    \caption{\textit{Roofline model} del código disperso}
    \label{fig:roofline_sparse_details}
\end{figure}

Teniendo en cuenta que los tiempos de ejecución de una red dispersa al 95\% son \textasciitilde25\% superiores a los de la red completamente conexa, aún teniendo que procesar un 95\% menos de datos, se puede realizar una estimación del número teórico de FLOP necesarios para las operaciones de multiplicación.

\subsubsection{Aproximación teórica}
Tal como se explica en la Sección \ref{sec:multiplicacion_point_to_point}, teniendo en cuenta que una operación FMA realiza dos FLOP, se puede concluir que el número de FLOP para la multiplicación de dos matrices $d\times D$ será $2 \cdot \#nz \cdot k$.

\subsubsection{Estimación del rendimiento}
Teniendo en cuenta que las matrices dispersas de pesos en el código disperso tienen una densidad del 5\% (o lo que es lo mismo, una \textit{sparsity} del 95\%), desde un punto de vista teórico se puede concluír que se deberían realizar un 95\% menos de operaciones en punto flotante.

Sin embargo esta reducción en el número de operaciones necesarias, debido a múltiples factores como el principio de localidad, estructura interna a la hora de almacenar la matriz, estructuras de control, así como optimizaciones internas de la propia librería, no se corresponde con una disminución del tiempo de ejecución, sino más bien todo lo contrario. Esta reducción en el número de FLOPS no lo es sin embargo en intensidad aritmética, puesto que también se cuenta con menor número de bytes de datos, resultando en un hipotético descenso en el eje $y$ en el modelo.

Por esta razón, el modelo \textit{roofline} resultante, que el programa Intel Advisor es incapaz de generar, se vería similar a lo que se puede observar en la Figura \ref{fig:roofline_sparse_estimado}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{img/roofline_sparse_estimado.png}
    \caption{Estimación del \textit{roofline model} del código disperso}
    \label{fig:roofline_sparse_estimado}
\end{figure}

Estos decepcionantes resultados, sin embargo, no son tan desalentadores, ya que implica que existe un amplio margen de mejora mediante análisis estático de código y generación de instrucciones FMA y de control \textit{ad-hoc}, que incluso pueden ser vectorizadas mediante el empleo de la herramienta MACVETH\footnote{\url{https://github.com/UDC-GAC/MACVETH}}.
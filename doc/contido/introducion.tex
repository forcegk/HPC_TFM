\chapter{Introducción}
\label{chap:introducion}

\lettrine{E}{n} este capítulo se expone la motivación de este \acrlong{tfm} sus objetivos y la estructura de la memoria.

\section{Motivación}
\label{sec:motivacion}
La computación de altas prestaciones (\acrshort{hpc}, o \acrlong{hpc}) y la \acrlong{ia} (\acrshort{ia}) hoy en día están en boca de todos. Esta última es sin duda la más espectacular y que más alegrías está dando, tanto al público general como al sector de la investigación.
Modelos que realizan tareas como conducción autónoma o cración instantánea de verdaderas obras de arte, así como muy convincentes ensayos en múltiples lenguas, nos hacen ver que las redes neuronales, lejos de estar en declive, están evolucionando a enorme velocidad, y con ello sus posibles aplicaciones presentes y futuras. Debido a este fuerte desarrollo de sistemas con inteligencia artificial, y la correspondiente presente y futura entrada de este tipo de modelos en el \textit{mainstream}, son cada vez más necesarias soluciones de bajo consumo en una economía de escala.

\section{Objetivos}
\label{sec:objetivos}
El objetivo principal de este trabajo es el análisis y una posible mejora de los tiempos de ejecución de modelos de inteligencia artificial en fase de inferencia, así como de su consumo energético. En esta memoria se realiza una breve introducción a una de las arquitecturas de redes neuronales más simples y su implementación de bajo nivel. En base a esta se analiza el rendimiento de las mismas y se proponen mejoras, así como una Prueba de Concepto de las mismas. En un siglo XXI donde el producto de matrices es uno de los algoritmos más fuertemente optimizados y trabajados de la historia de la computación, se propone una implementación de producto de matrices para redes neuronales dispersas mediante una arquitectura punto a punto (\textit{point-to-point} o p2p), y se compara su rendimiento con una de las librerías de matrices dispersas más empleadas.

Esta comparación de rendimiento, debido al tipo de código empleado y para obtener una mejor métrica de dónde se ha de incidir en el futuro, se debe realizar con las más que habituales gráficas de \textit{speedup} complementadas con modelos \textit{roofline}.

Los objetivos principales de este trabajo se cumplen durante el desarrollo de este trabajo, puesto que se debe obtener una comprensión detallada de funcionamiento a bajo nivel de este tipo de redes, su implementación en \acrlong{tf}, implementación alternativa en código C con \texttt{OpenBLAS}, \texttt{librsb} y \textit{point-to-point}, así como generación automatizada de dicho código.

\section{Estructura}
\label{sec:estructura}
En esta memoria se tratan las redes neuronales desde un punto de vista de implementación y matemático, se perfilan y se proponen mejoras. Comenzando por el Capítulo \ref{chap:conceptos_basicos}, se tratan los conceptos más básicos de las redes neuronales \textit{feed-forward}, se detalla la implementación a alto nivel, y se presentan conceptos que se citarán a lo largo del resto de la memoria. En el siguiente capítulo, se tratan los múltiples usos reales de las redes neuronales con diferentes arquitecturas pasadas y presentes, siendo un ejemplo de estas últimas las redes generativas adversarias y los \textit{transformers}, entre otros. En ese mismo capítulo se analizan los perfilados conducidos en las principales fuentes bibliográficas, para poner el foco en el exponencial aumento del consumo de energía que conllevarán estas redes en el futuro próximo. Como una posible solución se propone el podado de redes, junto a una Prueba de Concepto que demuestre la viabilidad de esta aproximación.

El funcionamiento de esta Prueba de Concepto (\textit{\acrlong{poc}} o \acrshort{poc}) se detalla en el Capítulo \ref{chap:desarrollo_poc}, donde se muestran, con un esquema de desarrollo incremental, los objetivos y pasos que se han seguido durante la creación de esta \acrshort{poc}.

Tras el desarrollo de esta Prueba de Concepto, se miden los rendimientos y se perfilan las tres implementaciones, comparando así redes densas basadas en \texttt{OpenBLAS}, con las dispersas basadas por un lado en \texttt{librsb}, y por otro en una arquitectura \textit{point-to-point} sin el empleo de librerías externas.

Tras todo esto, se termina con las conclusiones extraídas de este proceso, cómo este se relaciona con la titulación, las líneas futuras de investigación, así como las disciplinas empleadas y habilidades aprendidas durante la realización del \acrshort{tfm}. Por último, se incluye una lista de referencias bibliográficas, acrónimos y glosario.
